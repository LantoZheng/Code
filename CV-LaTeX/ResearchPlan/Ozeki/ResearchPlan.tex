% Clean XeLaTeX document for the Research Plan
\documentclass[12pt,a4paper]{article}
% Use fontspec for XeLaTeX
\usepackage{fontspec}
% Use a common system font; change if you prefer another installed font
\setmainfont{Times New Roman}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{setspace}
\onehalfspacing
\usepackage{microtype}
% Use biblatex for bibliography management with biber backend
\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{csquotes}
\addbibresource{ResearchPlan.bib}

% Bibliography: use biblatex + biber. Run sequence: xelatex -> biber -> xelatex -> xelatex
% Citations in the document use \cite{key} and will be rendered by biblatex.
\title{Research Plan}
\author{Xiaoyang Zheng}
\date{\today}

\begin{document}
\maketitle
\begin{center}
    \large \textbf{Intelligent Adaptive Optics for Deep-Tissue Hyperspectral Stimulated Raman Scattering Microscopy}
\end{center}
\sloppy

\begin{abstract}
This research proposes the development of an Intelligent Adaptive Optics (iAO) workflow for hyperspectral Stimulated Raman Scattering (SRS) microscopy to enable deep-tissue, label-free chemical imaging. The project integrates adaptive optics hardware—specifically, spatial light modulator-based wavefront control—with machine learning algorithms to address three interconnected objectives:
\begin{enumerate}
  \item Real-time, single-shot wavefront estimation and control employing a deep predictive controller (targeting sub-100 ms latency),
  \item AI-driven image restoration and hyperspectral unmixing to enhance effective signal-to-noise ratio (SNR) and chemical contrast, and
  \item Semi-supervised, uncertainty-aware segmentation and analysis to extract biologically relevant metrics from large, sparsely labeled datasets.
\end{enumerate}
The synergistic integration of these components is expected to restore diffraction-limited performance in highly scattering biological samples, extend the achievable imaging depth of the Ozeki Laboratory's hyperspectral SRS systems, and provide robust computational frameworks for the analysis of resulting high-dimensional datasets.
\end{abstract}

\section{Research Background}
Stimulated Raman Scattering (SRS) microscopy provides label-free chemical contrast by probing intrinsic molecular vibrations, achieving imaging speeds orders of magnitude faster than spontaneous Raman techniques \cite{cheng2015,v_w_freudiger2008}. The key advantages of SRS—including high chemical selectivity, linear concentration dependence, and three-dimensional optical sectioning capability—render it particularly well-suited for live biological imaging applications \cite{tipping2024}.

The Ozeki Laboratory has established expertise in three principal areas: (1) rapidly tunable laser sources for hyperspectral SRS imaging \cite{ozeki2019}, (2) enhanced detection sensitivity through quantum-enhanced approaches \cite{ozeki2020b}, and (3) integration of SRS with microfluidics and machine learning methodologies for high-throughput analysis \cite{ozeki2012,ozeki2020a}.

However, when imaging deep within tissue, sample-induced optical aberrations significantly degrade both focal quality and SRS signal strength, thereby imposing practical constraints on achievable imaging depth \cite{ji2020}. Adaptive optics (AO) addresses such aberrations through active wavefront correction, with spatial light modulators (SLMs) representing the most commonly employed devices for implementing phase corrections and holographic beam shaping \cite{papadopoulos2008}.
 
Although classical iterative sensorless AO algorithms—such as Zernike-mode hill-climbing—can improve signal quality, they exhibit two fundamental limitations: insufficient speed for imaging dynamic biological processes, and inability to leverage the rich correlations inherent in hyperspectral datasets. Recent advances in machine learning (ML) offer complementary strategies, including predictive single-shot wavefront correction, learned priors for image denoising and spectral unmixing, and semi-supervised or weakly-supervised segmentation frameworks that substantially reduce manual annotation requirements while providing calibrated uncertainty estimates. Consequently, the integration of the Ozeki Laboratory's established hardware capabilities with ML-driven control and analysis algorithms represents a strategic advancement with potentially transformative impact. The seamless incorporation of these AI methodologies directly into the AO-SRS imaging workflow constitutes the central innovation of the proposed research.

\section{Problem Statement}
Prior work has demonstrated the feasibility of combining AO with SRS microscopy through modal correction techniques and hill-climbing search strategies to enhance signal strength and spatial resolution \cite{wang2024a}. Nevertheless, these classical iterative control algorithms rely on sequential testing of correction patterns, rendering them insufficiently fast to compensate for dynamic aberrations characteristic of living tissue \cite{booth2006,tao2013}.

Moreover, hyperspectral SRS datasets acquired at significant depths are inherently large in size and exhibit substantial noise, presenting considerable challenges for conventional image processing workflows. Reliable extraction of chemical distribution maps and cellular morphological features typically necessitates extensive manual annotation—a labor-intensive and time-consuming process. This context presents a compelling opportunity to leverage deep learning methodologies both for intelligent optical control and for sophisticated analysis of the enhanced, aberration-corrected data generated by an iAO-enabled system.

\section{Proposed Work and Objectives}
The proposed research aims to develop a predictive, single-shot AO controller based on deep learning architectures, seamlessly integrated with a semi-supervised hyperspectral analysis pipeline. The specific research objectives are:
\begin{enumerate}
  \item To integrate a polychromatic AO module into an existing SRS microscope platform and establish a classical iterative control baseline for performance benchmarking.
  \item To develop and rigorously validate a convolutional neural network (CNN)-based predictive AO controller capable of single-shot wavefront estimation and correction.
  \item To deploy the intelligent AO-SRS system for biological imaging applications and implement a semi-supervised segmentation workflow tailored for hyperspectral datasets.
\end{enumerate}

To systematically address the multidisciplinary nature of this project, the research is organized into three interconnected technical tracks that will proceed concurrently with regular integration milestones:
\begin{itemize}
  \item \textbf{Track A (Hardware \& Optics):} SLM integration into the optical path, polychromatic relay optical design, system calibration, and establishment of a classical AO baseline employing Zernike modal control.
  \item \textbf{Track B (AI for Real-time Control):} Training dataset generation, development of CNN regression models for predicting corrective Zernike coefficients or direct SLM phase maps from single-shot images, latency optimization, and hardware-in-the-loop validation.
  \item \textbf{Track C (AI for Data Analysis):} Implementation of hyperspectral denoising and unmixing algorithms, development of semi-supervised segmentation frameworks (e.g., Mean Teacher architecture with uncertainty quantification), and creation of interpretability and visualization tools for end-user accessibility.
\end{itemize}

\section{Methods}
\textbf{Phase 1: System Integration and Iterative Baseline Establishment}
\begin{itemize}
  \item An LCoS-SLM will be integrated into the SRS optical path at a plane conjugate to the back focal plane of the objective lens. The relay optics will be designed to minimize chromatic dispersion between pump and Stokes beams. System alignment and optical performance will be validated using fluorescent bead phantoms and tissue-mimicking scattering media.
  \item A classical sensorless AO baseline implementing Zernike-mode hill-climbing and modal optimization algorithms will be developed in Python. This baseline system will serve dual purposes: (1) generation of ground-truth training datasets for Phase 2 model development, and (2) establishment of performance benchmarks against which the iAO controller will be evaluated \cite{papadopoulos2008,tao2013}.
\end{itemize}

\textbf{Phase 2: Predictive Deep Learning Controller and Image Restoration}
\begin{itemize}
  \item \textit{Data collection:} The Phase 1 apparatus will be employed to systematically apply thousands of randomized aberration patterns—comprising varied Zernike coefficient sets or pre-calibrated SLM phase maps—while recording the corresponding degraded SRS images across multiple spectral channels. Training data will consist of paired observations: (aberrated image, known corrective phase) and, where feasible, (aberrated image, aberration-free reference image).
  \item \textit{Model architecture:} Two complementary CNN architectures will be developed and compared: (1) an image-to-vector regression network for predicting Zernike modal coefficients, and (2) a U-Net-based image-to-phase network for direct prediction of SLM phase correction maps. Candidate encoder architectures include ResNet-derived networks (offering robustness to noise), lightweight MobileNet-inspired designs (enabling low-latency inference), and spatiotemporal or 3D convolutional extensions for volumetric imaging scenarios.
  \item \textit{Training methodology:} The training protocol will incorporate extensive data augmentation—including simulated photon noise, motion blur, and spectral variability—alongside domain randomization techniques to enhance model generalization from phantom samples to biological tissue. Curriculum learning strategies, progressing from coarse to fine Zernike mode prediction, will be implemented. Model performance will be assessed through both optical metrics (residual wavefront error) and computational efficiency (inference latency on target hardware platforms).
  \item \textit{Image restoration:} Learned denoising networks and hyperspectral unmixing algorithms—implemented as 2D/3D U-Nets or spectral-domain CNNs—will be developed to exploit the improved point spread functions resulting from wavefront correction. These networks will aim to enhance effective SNR and extract quantitative concentration maps for principal biological constituents, including lipids, proteins, and nucleic acids.
\end{itemize}

\textbf{Phase 3: Biological Application and Semi-Supervised Analysis}
\begin{itemize}
  \item The fully integrated iAO-SRS system will be applied to biologically relevant model systems, including three-dimensional tumor spheroids, organoids, and thick ex vivo brain tissue sections. Systematic benchmarking will quantify depth-dependent improvements in point spread function quality, image contrast, and chemical mapping fidelity relative to conventional SRS imaging.
  \item \textit{Semi-supervised segmentation framework:} An initial U-Net segmentation model will be trained on a limited set of manually annotated superficial image regions. The model will then be extended to deeper tissue regions through pseudo-labeling techniques and Mean Teacher-style consistency regularization. Uncertainty estimation methods—such as Monte Carlo dropout or deep ensemble approaches—will be integrated to provide confidence metrics, enabling downstream analyses to appropriately weight predictions according to their reliability.
  \item \textit{Active learning protocol:} An iterative active learning loop will be established to strategically select regions for manual annotation based on model-predicted uncertainty and expected information gain. This approach aims to minimize human annotation effort while maximizing segmentation performance in challenging deep-tissue regions characterized by high scattering and low signal-to-noise ratios.
\end{itemize}

\section{Evaluation and Success Criteria}
The success of the proposed research will be evaluated through the following quantitative metrics:
\begin{itemize}
  \item \textit{Optical performance:} Enhancement in SRS signal intensity and restoration of point spread function quality will be quantified through measured improvements in Strehl ratio and reductions in PSF full-width at half-maximum, evaluated as functions of imaging depth.
  \item \textit{Control system performance:} The wavefront correction accuracy will be assessed by residual wavefront error (root-mean-square Zernike coefficients), correction fidelity relative to the classical baseline, and controller latency (design target: $<$100 ms for single-shot inference, inclusive of data transfer overhead).
  \item \textit{Data analysis performance:} Image restoration quality will be evaluated using peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). Segmentation accuracy will be assessed via Dice similarity coefficient and intersection-over-union (IoU) metrics on held-out test datasets. The calibration quality of uncertainty estimates will be quantified through expected calibration error.
  \item \textit{System-level impact:} The practical utility of the iAO system will be demonstrated through quantifiable increases in achievable imaging depth (e.g., fold-increase in depth for diffraction-limited imaging relative to non-AO controls) and biological validation studies yielding quantitative analyses of metabolic distributions and cellular features that are inaccessible without intelligent adaptive optics.
\end{itemize}

%\section{Timeline and Milestones}
%A representative 24-month timeline with quarterly milestones:
%\begin{enumerate}
%  \item Months 0--3: Optical design, procurement (SLM, relay optics), and software baseline implementation.
%  \item Months 4--8: Phase 1 integration, phantom validation, and classical AO baseline measurements.
%  \item Months 9--14: Large-scale data collection and Phase 2 model development (CNNs / U-Nets). Initial hardware-in-the-loop tests.
%  \item Months 15--18: Low-latency optimization, deployment on accelerator (GPU/edge TPU), and closed-loop single-shot control demonstrations.
%  \item Months 19--24: Biological imaging campaigns, semi-supervised segmentation, active learning cycles, and preparation of manuscripts and open-source code / trained models.
%\end{enumerate}

%Milestones will include: working SLM-integrated microscope (M1), trained predictive AO model with $\geq 90\%$ baseline correction accuracy on phantoms (M2), real-time closed-loop demo (M3), and biological proof-of-concept with quantifiable depth gains (M4).

\section{Risks and Mitigation}
Several technical and programmatic risks have been identified, along with corresponding mitigation strategies:
\begin{itemize}
  \item \textit{Hardware integration challenges:} Achieving precise chromatic alignment between pump and Stokes wavelengths may necessitate multiple iterations of optical design and alignment. \textbf{Mitigation:} The relay optics will be designed with modularity to facilitate adjustments; spare alignment components will be procured in advance; and rigorous validation using phantom samples will be conducted prior to biological imaging experiments.
  \item \textit{Model generalization limitations:} Deep learning models trained exclusively on phantom data may exhibit reduced performance when applied to heterogeneous biological tissues. \textbf{Mitigation:} Training protocols will incorporate domain randomization techniques; datasets will include both phantom and ex vivo tissue samples; and transfer learning approaches will be employed using small, carefully annotated biological datasets to bridge the domain gap.
  \item \textit{Real-time latency constraints:} Achieving the target closed-loop latency of $<$100 ms may prove challenging without dedicated hardware acceleration. \textbf{Mitigation:} Initial prototyping will leverage GPU resources, followed by systematic optimization through network pruning, weight quantization, or deployment on specialized edge computing platforms (e.g., NVIDIA Jetson, Google Coral) as required.
  \item \textit{Annotation bottleneck:} The scarcity of manually annotated deep-tissue training data may impede progress in segmentation algorithm development. \textbf{Mitigation:} Semi-supervised learning frameworks, active learning protocols with uncertainty-guided sample selection, and curriculum learning strategies will be employed to minimize dependence on extensive manual annotation.
\end{itemize}

%\section{Data Management and Reproducibility}
%I will follow reproducible research practices to make methods and results verifiable and reusable:
%\begin{itemize}
%  \item Data: raw and processed hyperspectral stacks, calibration measurements, and example phantom datasets will be stored with descriptive metadata and will be archived in a lab data repository. Sensitive biological data will be handled under standard institutional policies.
%  \item Code and models: acquisition control scripts, baseline AO code, model training and inference scripts, and trained model weights will be released via a public repository (e.g., GitHub) with a permissive license and a small README showing how to reproduce key results.
%  \item Evaluation: I will publish quantitative evaluation scripts and example notebooks that compute PSNR/SSIM, Strehl ratios, Dice/IoU, and latency measurements so reviewers can reproduce reported metrics.
%  \item Documentation: a short `README.md` will document software/hardware dependencies, recommended GPU drivers, and step-by-step build/run instructions (including the latex build steps used here).
%\end{itemize}

\newpage
\printbibliography

\end{document}

