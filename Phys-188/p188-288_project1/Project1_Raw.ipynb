{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"Project1.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c325f4FMYg_"
   },
   "source": [
    "# Project 1\n",
    "\n",
    "# <em> Optimization, Monte Carlo Problems, pocoMC </em>\n",
    "<br>\n",
    "\n",
    "Welcome to your first project. Unlike the homework, you will generally receive less guidance on projects but are still expected to do the work required. This project primarily covers sampling and the use of MCMC methods. You will be analyzing some data using a number of methods for parameter inference.\n",
    "\n",
    "Though there will be physics involved in these projects, the physical quantities and equations are generally given to you, while you are expected to use the physics equations to produce data analysis results. Accordingly, autograder will be less through for these questions, and most of the project will be graded manually, so make sure you include documentation with your code!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtAJ_i6WMYhE"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1356,
     "status": "ok",
     "timestamp": 1730176181937,
     "user": {
      "displayName": "Henry Liu",
      "userId": "02481607702682765299"
     },
     "user_tz": 420
    },
    "id": "0czaqoxFMYhE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQpaK7wZMYhH"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heUEoQxa9XQ3"
   },
   "source": [
    "# Problem 1: Constraints on Galaxy Clustering\n",
    "\n",
    "Parameter constraints methods are used in many physical problems. In this project, we consider the use of two different methods, Maximum Likelihood Estimation (MLE) and Markov Chain Monte Carlo (MCMC), to constrain Halo Occupation Distribution (HOD) models.\n",
    "\n",
    "Here, we consider constraints on the Halo Occupation Distribution (HOD) models for projected galaxy clustering.\n",
    "\n",
    "HOD models are used to interpret the relation of galaxy distributions in large scale structure to the dark matter distribution. They describe related properties of the galaxy distribution within a dark matter halo: The probability distribution of the number of galaxies within that halo in relation to the mass of the halo, the distribution in space of galaxies withing the DM halo and the distribution in velocity of galaxies in the halo.\n",
    "\n",
    "Traditional HOD models make two assumptions: (1) that all galaxies reside in dark matter haloes and are biased tracers of the underlying matter density and (2) galaxies occupy halos **only** as a function of the halo mass $M_{vir}$. However, these are not necessarily correct, an effect called *halo assembly bias* could be significant. They also add additional parameters to the HOD model. In this problem, we will infer some HOD model parameters given some input data using MCMC.\n",
    "\n",
    "First, we will describe the parameters. HOD models usually treat central galaxies and satellite galaxies in a halo separately, and the occupation statistics of the central galaxy will be different than the occupation statistics of the satellite galaxies in the halo.\n",
    "The average number of central galaxies in a halo of mass $M$ is given by\n",
    "\n",
    "$$\\langle N_{cen}\\rangle_{M} = \\frac12 \\left[1 + \\mathrm{erf}\\left( \\frac{\\log M - \\log M_{\\mathrm{min}}}{\\sigma_{\\log M}}\\right)\\right]$$\n",
    "\n",
    "While the average number of satellite galaxies in a halo of mass $M$ is given by\n",
    "\n",
    "$$\\langle N_{sat}\\rangle_{M} = \\left[\\frac{M-M_0}{M_1}\\right]^\\alpha$$\n",
    "\n",
    "The parameter $M_{\\mathrm{min}}$ describes the halo mass at which the halo has 50\\% probability to host a central galaxy, while the parameter $\\sigma_{\\log M}$ can be considered the rate at which the likelihood of the central galaxy to exist increases alongside halo mass. The parameter $M_1$ is the halo mass at which an average halo would host one satellite galaxy, while $M_0$ is the halo mass at which an average halo would not have a satellite galaxy. $\\alpha$ is the power-law exponent relation parameter between the number of satellites and the halo mass. Lastly, we also have the two parameters $A_{cen}$ and $A_{sat}$, quantify describe the halo assembly bias.\n",
    "\n",
    "In summary, we take a likelihood model of the Halo Occupation Distribution and infer the following parameters:\n",
    "$$[\\log(M_{\\mathrm{min}}), \\sigma_{\\log M}, \\log(M_0), \\log(M_1), \\alpha, A_{cen}, A_{sat}]$$\n",
    "\n",
    "(This problem is based on the following paper: https://arxiv.org/pdf/1606.07817.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VE-bW2VA9mE8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import corner\n",
    "from scipy.stats import uniform\n",
    "from tabcorr import TabCorr\n",
    "from halotools.empirical_models import HodModelFactory\n",
    "from halotools.empirical_models import AssembiasZheng07Cens\n",
    "from halotools.empirical_models import AssembiasZheng07Sats\n",
    "\n",
    "import numdifftools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFlvABvc9lm2"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "halotab = TabCorr.read('./zentner19data/bolplanck.hdf5')\n",
    "cens_occ_model = AssembiasZheng07Cens()\n",
    "sats_occ_model = AssembiasZheng07Sats()\n",
    "model = HodModelFactory(centrals_occupation=cens_occ_model,\n",
    "                        satellites_occupation=sats_occ_model)\n",
    "\n",
    "n_obs = 6.37e-3\n",
    "n_err = 0.75e-3\n",
    "wp_obs = np.genfromtxt('./zentner19data/wp_dr72_bright0_mr20.0_z0.106_nj400')[:, 1] # Observed galaxy two-point function\n",
    "wp_cov = np.genfromtxt('./zentner19data/wpcov_dr72_bright0_mr20.0_z0.106_nj400') # Observed galaxy two-point function covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c25rNVQ9lxs"
   },
   "source": [
    "First, we would like to define priors on the model parameters. Since we don't know too much beyond the range of the priors, we will define uniform priors for each parameter as given:\n",
    "\n",
    "\\begin{array}{ c|c }\n",
    " \\text{Parameter} & \\text{Prior Interval} \\\\\n",
    " \\hline\n",
    " \\log(M_{\\mathrm{min}}) & [9.0, 14.0] \\\\\n",
    " \\sigma_{\\log M} & [0.01, 1.5]\\\\\n",
    " \\log(M_0) & [9.0, 14.0]\\\\\n",
    " \\log(M_1) & [10.7, 15.0]\\\\\n",
    " \\alpha & [0.0, 2.0]\\\\\n",
    " A_{cen} & [-1.0, 1.0] \\\\\n",
    " A_{sat} & [-1.0, 1.0]\n",
    "\\end{array}\n",
    "\n",
    "Furthermore, to constrain parameters, we need a description of the likelihood function of parameters. Here, following the accompanying paper to this problem, we define the likelihood of the form $\\mathit{L} \\propto e^{-\\chi^2/2}$, with the $\\chi^2$ given as\n",
    "\n",
    "$$\\chi^2 = \\Delta w_{p,i}\\, [C^{-1}]_{ij}\\, \\Delta w_{p, j} + \\frac{(n_g^{\\text{mock}} - n_g^{\\text{obs}})^2}{\\sigma^2_n}$$\n",
    "\n",
    "with $ \\Delta w_{p,i} \\equiv w_p^{\\text{mock}}(r_{p, i}) -  w_p^{\\text{obs}}(r_{p, i})$ being the difference between the projected and observed two point correlation functions $w_{p}(r_{p,i})$. The observed $w_p^{\\text{obs}}$ is given to you while the predicted $w_p^{\\text{mock}}$ is received from the model (see below). The $r_{p,i}$ represents the 12 $r$ bins, and you can see that the given $w_p$ are indeed 12 dimensional. $C$ is the covariance matrix of the measurements (also given), and the term $\\frac{(n_g^{\\text{mock}} - n_g^{\\text{obs}})^2}{\\sigma^2_n}$ is the contribution from the difference between the predicted and measured galaxy number densities. ($n_g^{\\text{mock}}$ is given by the model, while $\\sigma_n$ and $n_g^{\\text{obs}}$ are given from the data).\n",
    "\n",
    "## **Problem Goals**\n",
    "\n",
    "Using the likelihood function and priors given,  you are expected to compute constraints on the halo parameters in two different methods: first, using a **Maximum Likelihood Estimate (MLE)** approach where you assume the data is Gaussian, then using the **Metropolis-Hastings algorithm** for **Markov Chain Monte Carlo (MCMC)**. You are then expected to compare the results from these two methods and briefly discuss the differences. Lastly, compare the results to Figure 6 from https://arxiv.org/pdf/1606.07817.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "996DeRqzulWN"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Part 1. Define the Log Likelihood function\n",
    "\n",
    "\n",
    "Given a set of data $X$ and a model $M(\\theta)$ on this data with parameters $\\theta$, we can define the likelihood function as the probability that the model with parameters $\\theta$ describes the data:\n",
    "$$L(\\theta; X) = P(X | \\theta)$$\n",
    "\n",
    "Usually, we work in the log space as the log-likelihood is easier to define and optimize. As the log function is monotonic, anything that yields the maximum or minimum log-likelihood would then also yield the maximum or minimum likelihood. The log-likelihood is defined as\n",
    "$$\\ell(\\theta;X) = \\log(L(\\theta; X))$$\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> <i> Using this chi square given above, define the priors and the log likelihood function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmIeYVNs0bK6",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define log-likelihood function\n",
    "names = ['logMmin', 'sigma_logM', 'logM0', 'logM1', 'alpha',\n",
    "         'mean_occupation_centrals_assembias_param1',\n",
    "         'mean_occupation_satellites_assembias_param1']\n",
    "bounds = ...\n",
    "\n",
    "def log_likelihood(theta):\n",
    "\n",
    "    theta = theta.copy() # copy the x so as to not change the original\n",
    "    model.param_dict.update(dict(zip(names, theta))) # update model with input parameters\n",
    "    n_mod, wp_mod = halotab.predict(model) # get predicted results\n",
    "    \n",
    "    return ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "EyUUkWf30hyK"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Part 2. Maximum Likelihood Estimate / Maximum A Posteriori\n",
    "\n",
    "Given a log likelihood function $\\ell(\\theta; X)$, in the frequentist set-up we can calculate the maximum likelihood estimate (MLE) of our parameters $\\theta$ by finding the MLE best fit parameters $\\widehat{\\theta}$\n",
    "\n",
    "$$\\widehat{\\theta} = \\mathrm{argmax}_\\theta \\; \\ell(\\theta; X)$$\n",
    "\n",
    "However, in this problem, we don't just want to find the *best fit parameters* $\\widehat{\\theta}$, but we also want to find the *uncertainties* of these best fit parameters, as well as the covariances of these parameters and their distribution in parameter space.\n",
    "\n",
    "The Fisher Information Matrix could help us find the parameter covariance. Recall that the Fisher Matrix is defined as\n",
    "$$F_{ij} = - \\left\\langle \\frac{∂^2}{∂\\theta_i∂\\theta_j} \\ell(\\theta; X)\\right\\rangle_{\\theta_{MLE}}$$\n",
    "(Note that as described in the lecture notes, the fisher matrix is evaluated at the best-fit MLE parameters) The Fisher matrix also has the property such that its inverse yields the covariance matrix.\n",
    "$$C^{-1} = F$$\n",
    "\n",
    "To compute the Fisher information, we make use of the Hessian matrix of the log-likelihood. This is defined as\n",
    "$$H_{ij} = \\frac{∂^2}{∂\\theta_i∂\\theta_j} \\ell(\\theta; X)$$ which means that the Fisher Information matrix is the negative expectation of the Hessian\n",
    "$$F = -\\left\\langle H \\right\\rangle$$\n",
    "\n",
    "Note: this is only true in the assumption of large, independent samples from the likelihood, when distribution on $\\theta$ exhibits asymptotic normality.\n",
    "$$\\hat{\\theta} \\sim N\\left(\\theta, \\frac{F(\\theta)^{-1}}{n} \\right)$$\n",
    "By the Laplace approximation, this indicates that the Hessian would converge to the Fisher Information Matrix.\n",
    "\n",
    "In practice, this symbolizes that the Hessian of the log likelihood can be used to approximate the Fisher matrix. Under the assumption that the parameter space is Gaussian distributed then, we can use the Hessian of the likelihood to calculate the covariance matrix and plot Gaussian stair plots showing the distribution in parameter space.\n",
    "\n",
    "**The goal of this section is to accomplish this task. Find the parameters that maximize the log likelihood within the prior bounds given above, and plot the parameter distributions for the parameters given, using the prior bounds as bounds of each of the stair plots.** To help in the plotting part, a skeleton stair plot code is provided, but you will have to modify the code to fit the parameters. (Note that technically, we are conducting Maximum A Posteriori (MAP) estimation of parameters as we're incorporating the uniform priors given earlier)\n",
    "\n",
    "Hints:\n",
    "\n",
    "1.   `scipy.optimize` allows the use of optimization methods with bounds on optimization parameters. Read the documentation and use a method that is able to do bounded optimization on the likelihood.\n",
    "2.   To compute the Hessian on the log-likelihood function, use numerical finite difference methods and not automatic differentiation methods. This is because the likelihood function you constructed in Part 1 utilises packages which are not automatically differentiable by autograd or pytorch. The package `numdifftools` contains the function `numdifftools.Hessian` which will help you in this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Use these labels for the parameters. Make sure you use the parameters in this order for the autograder!\n",
    "\n",
    "labels = [r'$\\log M_{min}$', r'$\\sigma_{\\log M}$',r'$\\log M_0$',r'$\\log M_1$',r'$\\alpha$',r'$A_c$',r'$A_s$'] \n",
    "\n",
    "opt_p = ...\n",
    "\n",
    "print(opt_p)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    print(r\"MLE value of %s = %.5f\" %(..., ...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Triangle Plot (Original skeleton code by Nicholas Kern)\n",
    "# Only a suggestion. You can create your own if you wish.\n",
    "\n",
    "fig, axes = plt.subplots(6,6,figsize=(12,12))\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "p_tex = np.array([r'$H_0$', r'$\\Omega_bh^2$',r'$\\Omega_ch^2$',r'$n_s$',r'$10^9 A_s$',r'$\\tau$'])\n",
    "\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        ax = axes[i, j]\n",
    "        if j > i:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        elif i == j:\n",
    "            # diagonal part\n",
    "            ax.grid(True)\n",
    "            xarr = ...\n",
    "            yarr = ...\n",
    "            ax.plot(...)\n",
    "            ax.set_xlim(...)\n",
    "            ax.set_xticks(...)\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_xticklabels([])\n",
    "        else:\n",
    "            # off-diagonal part\n",
    "            ax.grid(True)\n",
    "\n",
    "            # Covariance matrix\n",
    "            CovM = ...\n",
    "\n",
    "            # Get eigenvalue/vector using svd\n",
    "            eigvec, eigval, u = np.linalg.svd(CovM)\n",
    "\n",
    "            # Get Semimajor/minor axes of the ellipse\n",
    "            semimaj = np.sqrt(eigval[0])*2.\n",
    "            semimin = np.sqrt(eigval[1])*2.\n",
    "\n",
    "            # Rotation angle of the ellipse\n",
    "            theta = np.arctan(eigvec[0][1]/eigvec[0][0])\n",
    "\n",
    "            # Create ellipses\n",
    "            ell = mpl.patches.Ellipse(xy=[...], width=1.52*semimaj, height=1.52*semimin, angle = theta*180/np.pi, facecolor = 'dodgerblue', edgecolor = 'royalblue', label = '68% confidence')\n",
    "            ell2 = mpl.patches.Ellipse(xy=[...], width=2.48*semimaj, height=2.48*semimin, angle = theta*180/np.pi, facecolor = 'skyblue', edgecolor = 'royalblue', label = '95% confidence')\n",
    "\n",
    "            ax.add_patch(ell2)\n",
    "            ax.add_patch(ell)\n",
    "\n",
    "            ax.set_ylim(...)\n",
    "            ax.set_xlim(...)\n",
    "            ax.set_xticks(...)\n",
    "            ax.set_yticks(...)\n",
    "\n",
    "\n",
    "        if j != 0:\n",
    "            ax.set_yticklabels([])\n",
    "        if i != 5:\n",
    "            ax.set_xticklabels([])\n",
    "        if j == 0 and i !=0:\n",
    "            ax.set_ylabel(p_tex[i], fontsize=10)\n",
    "            ax.set_yticklabels(...)\n",
    "            [tl.set_rotation(26) for tl in ax.get_yticklabels()]\n",
    "        if i == 5:\n",
    "            ax.set_xlabel(p_tex[j], fontsize=10)\n",
    "            ax.set_xticklabels(...)\n",
    "            [tl.set_rotation(26) for tl in ax.get_xticklabels()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DeyhfzMr3x6V"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Part 3. Markov Chain Monte Carlo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gKBF_fOK0YAK"
   },
   "source": [
    "In many cases, the likelihood or the posterior distribution over parameter space is highly non-Gaussian.\n",
    "When the Gaussian approximation is no longer sufficient for parameter distribution estimation and sampling, we can instead use Markov Chain Monte Carlo methods to do sampling to get a better estimate for the distribution of parameters.\n",
    "\n",
    "\n",
    "\n",
    "Here, we will use the `pocomc` package as an example case for parameter sampling estimation. Following the example, you are expected to conduct the same type of sampling with your own Metropolis-Hastings algorithm, and get the same resulting plot.\n",
    "\n",
    "So we define the priors as a list to the `pc.Prior` function, each item in the list should be a uniform random variable given by `uniform(lower, upper-lower)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pocomc as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFnaclKs9zsh",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define prior\n",
    "\n",
    "prior = pc.Prior([\n",
    "    uniform(9,14-9), # logMmin in [9,14]\n",
    "    uniform(0.01,1.5-0.01), # sigma_logM in [0.01,1.5]\n",
    "    uniform(9,14-9), # logM0 in [9,14]\n",
    "    uniform(10.7,15.0-10.7), # logM1 in [10.7,15.0]\n",
    "    uniform(0,2-0), # alpha in [0,2]\n",
    "    uniform(-1,+1-(-1)), # mean_occupation_centrals_assembias_param1 in [-1,+1]\n",
    "    uniform(-1,+1-(-1)), # mean_occupation_satellites_assembias_param1 in [-1,+1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215320,
     "status": "ok",
     "timestamp": 1727892017531,
     "user": {
      "displayName": "Henry Liu",
      "userId": "02481607702682765299"
     },
     "user_tz": 420
    },
    "id": "XMNdS9mIBsHn",
    "outputId": "2fa575ad-fc9f-434b-e1ef-9d0b73af453d",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Run pocoMC sampler (note that this should take ~ 5 minutes)\n",
    "\n",
    "sampler = pc.Sampler(prior=prior, likelihood=log_likelihood)\n",
    "\n",
    "sampler.run()\n",
    "\n",
    "samples, weights, logl, logp = sampler.posterior()\n",
    "\n",
    "logz, logz_err = sampler.evidence()\n",
    "\n",
    "print('logZ = ', np.round(logz,4), '+-', np.round(logz_err,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3161,
     "status": "ok",
     "timestamp": 1727892020690,
     "user": {
      "displayName": "Henry Liu",
      "userId": "02481607702682765299"
     },
     "user_tz": 420
    },
    "id": "iBucBwKSBsWE",
    "outputId": "af63e31d-d240-4579-82e2-db6fa393c7e6",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "corner.corner(samples, weights=weights, color='C0', smooth=1.0,\n",
    "              labels=['logMmin', 'sigma_logM', 'logM0', 'logM1', 'alpha', 'A_c', 'A_s'],\n",
    "              range=[(9,14),(0.01,1.5),(9,14),(10.7,15.0),(0,2),(-1,+1),(-1,+1)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1727892017532,
     "user": {
      "displayName": "Henry Liu",
      "userId": "02481607702682765299"
     },
     "user_tz": 420
    },
    "id": "t1LUVteqdTuM",
    "outputId": "6a467abb-fbae-4516-884c-3978f72bf9bf",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    print(\"%s = %.5f +/- %.5f\" %(..., ..., ...)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1727892017532,
     "user": {
      "displayName": "Henry Liu",
      "userId": "02481607702682765299"
     },
     "user_tz": 420
    },
    "id": "0OpJJJbVdVzR",
    "outputId": "bc8a69e9-de61-40f1-a01a-c0b41d389a70",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    print(\"%s : MLE values %.2f sigma away from MCMC mean\" %(..., ...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fp0O7y44YNaj"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "The results from this corner plot should look similar to your results from Part 2 using MLE. (If they don't, double check your log-likelihood function or your MLE code). We see however that the results are not quite Gaussian, especially given our priors. Our goal then is to reproduce this work using a simpler Markov Chain Monte Carlo algorithm: Metropolis Hastings.\n",
    "\n",
    "**Write your own MCMC algorithm using Metropolis Hastings, and sample from the posterior distribution and make a corner plot like the one above. Report the mean and standard deviations of your sampled parameters, and plot the convergences of your chains as a function of timestep. Lastly, discuss the differences your simple MCMC alrogithm have with the pocomc plot above, the MLE plot from Part 2 as well as Figure 6 from https://arxiv.org/pdf/1606.07817. Remember to leave comments in your code and explain your steps.**\n",
    "\n",
    "Hints:\n",
    "\n",
    "\n",
    "1.   Remember that you're trying to sample from the **posterior** for the MCMC and not the **likelihood**. Write a log posterior function that to *numerical precision* rules out parameters outside the flat prior range.\n",
    "2.   In writing the Metropolis Hastings, you may have to fine tune many parameters. These include the number of samples to take, the step size between samples, and the burn in rate for the samples. I suggest you write the code to make it easy to change these samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Problem 2: Predator - Prey Model\n",
    "\n",
    "\n",
    "**1) Motivation**\n",
    "\n",
    "Another application of a MC simulation is population dynamics. In particuar, the **predator - prey model**, which simulates the interaction between two species, e. g. wolfes and lambs was one of the early models that could explain periodic pattern in population sizes over time.<br> \n",
    "The goal of this exercise is to apply the MC simulation using the Gillespie algorithm to the predator - prey model and thereby gain more understanding of these methods.\n",
    "\n",
    "**2) Preparation**\n",
    "\n",
    "In its simplest version the predator - prey model consists of only three equations:<br>\n",
    "<br>\n",
    "- lambs $L$ double themselves with a rate $k_1$<br>\n",
    "  $L \\xrightarrow{k_1} 2\\,L$<br>\n",
    "- if a wolf $W$ meets a lamb $L$, it kills it and turns it into a new wolf with a rate $k_2$, such that<br>\n",
    " $L + W \\xrightarrow{k_2} 2\\,W$<br>\n",
    "- Wolfs can starve (whereas lambs don't, they can always eat grass). If they do not meet a lamb, they will die with the rate $k_3$<br>\n",
    " $W \\xrightarrow{k_3} \\Phi$<br>\n",
    "\n",
    "**3) Exercise**\n",
    "\n",
    "- a) Write a Python script using *def* that simulates the predator - prey model from above via a MC simulation using the Gillespie algorithm. Start with the following values: $L(t = 0) = W(t = 0) = 1000$, $k_1 = 10$, $k_2 = 0.01$ and $k_3 = 10$. Experiment with sligthly different values.\n",
    "- b) Plot $L(t)$ and $W(t)$, but also plot $L(W)$ vs $W(t)$. What do you observe?<br>\n",
    "- c) In reality a wolf does not immediately turn a lamb into another wolf, but rather uses the energy for maintaining its metabolism. This would add another equation like $W + L \\xrightarrow{k_4} W$ to the model. Also lambs can die by natural causes via $L \\xrightarrow{k_5} \\Phi$. Discuss (**no simulation is required!**) that adding these equations does not change the model at all. Apply what you know about rate equations.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def PredPrey(Lambs: int = 1000, Wolfes: int = 1000,\n",
    "             k1: float = 10, k2: float = 0.01, k3: float = 10,\n",
    "             Niter: int = 1000000):\n",
    "\n",
    "    ...\n",
    "\n",
    "    # Plotting code below: The function should plot the result, but not necessarily return anything\n",
    "\n",
    "    # First, plot L(t) and W(t):\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ...\n",
    "\n",
    "    plt.xlabel('time')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # Next, plot L(W):\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ...\n",
    "    plt.xlabel('Wolfes')\n",
    "    plt.ylabel('Lambs')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "PredPrey(Niter = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "PredPrey(Niter = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "PredPrey(Niter = 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Submit only the PDF to Gradescope! Do not submit the zip or ipynb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Physics188-288)",
   "language": "python",
   "name": "shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "p188_288_project1",
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.allclose(np.array(bounds[0]), np.array((9, 14)))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.allclose(np.array(bounds[4]), np.array((0, 2)))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(log_likelihood([11.5, 0.745, 11.5, 12.85, 1, 0, 0]), -554.5204, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(opt_p.x[5], 1)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(opt_p.x[2], 12.183, rtol=0.01)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(opt_p.x[-1], -0.06164, rtol=0.01)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
