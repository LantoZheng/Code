{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75677d52",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"HW_04.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736e53e-7a36-4e82-9caa-d20dcc2d1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9bc92c-0368-484b-821f-68e9305a5e4c",
   "metadata": {
    "id": "UnrhYaOSGTt0"
   },
   "source": [
    "## Homework 4\n",
    "\n",
    "## <em> Mixture Models and Expectation Maximization</em>\n",
    "<br>\n",
    "This notebook is arranged in cells. Texts are usually written in the markdown cells, and here you can use html tags (make it bold, italic, colored, etc). You can double click on this cell to see the formatting.<br>\n",
    "<br>\n",
    "The ellipsis (...) are provided where you are expected to write your solution but feel free to change the template (not over much) in case this style is not to your taste. <br>\n",
    "<br>\n",
    "<em>Hit \"Shift-Enter\" on a code cell to evaluate it.  Double click a Markdown cell to edit. </em><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb55d9-66f8-48c4-8a40-b27e3666a6ed",
   "metadata": {
    "id": "6B47XzZOGTt4"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea57257-e56a-4184-a08d-729791cdcf17",
   "metadata": {
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1729660006350,
     "user": {
      "displayName": "Henry Liu",
      "userId": "02481607702682765299"
     },
     "user_tz": 420
    },
    "id": "0UpkwBPmGTt4"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.integrate import quad\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53877912-74cf-4a8d-8bb5-4984532f34cb",
   "metadata": {
    "id": "7URXTVOWGTuK"
   },
   "source": [
    "#### Problem 1 - Mixture Model for Outliers\n",
    "\n",
    "Suppose we have data that can be fit to a linear regression, apart from a few outlier points. It is always better to understand the underlying generative model of outliers. <br><br>\n",
    "Consider the following dataset, relating the observed variables $x$ and $y$, and the error of $y$ stored in $\\sigma_y$. <br><br>\n",
    "We'll propose a simple linear model, which has a slope and an intercept encoded in a parameter vector $\\theta$. The model is defined as follows:\n",
    "\n",
    "$$\\hat{y} (x|\\theta) = \\theta_0 + \\theta_1 x$$\n",
    "Given this model, we can compute a Gaussian likelihood for each point:\n",
    "$$p(x_i, y_i, e_i|\\theta) \\propto \\exp \\left[ - \\frac{1}{2e_i^2} (y_i - \\hat{y} (x_i |\\theta))^2\\right]$$\n",
    "The total likelihood is the product of all the individual likelihoods. Computing this and taking the log, we have:\n",
    "$$\\log L(D |\\theta) = \\mathrm{const} - \\sum_i \\frac{1}{2e_i^2} (y_i - \\hat{y} (x_i |\\theta))^2 $$\n",
    "This should all look pretty familiar if you read through the previous post. This final expression is the log-likelihood of the data given the model, which can be maximized to find the $\\theta$ corresponding to the maximum-likelihood model. Equivalently, we can minimize the summation term, which is known as the loss:\n",
    "$$\\mathrm{loss} = \\sum_i \\frac{1}{2e_i^2} (y_i - \\hat{y} (x_i |\\theta))^2$$\n",
    "This loss expression is known as a squared (or L2) loss; here we've simply shown that the squared loss can be derived from the Gaussian log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b071bdc-88f3-4d0b-8b1d-4ee1e69cbca6",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1729660191570,
     "user": {
      "displayName": "Henry Liu",
      "userId": "02481607702682765299"
     },
     "user_tz": 420
    },
    "id": "v5XDsDmvGTuK"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "x = np.array([ 0,  3,  9, 14, 15, 19, 20, 21, 30, 35,\n",
    "              40, 41, 42, 43, 54, 56, 67, 69, 72, 88])\n",
    "y = np.array([33, 68, 34, 34, 37, 71, 37, 44, 48, 49,\n",
    "              53, 49, 50, 48, 56, 60, 61, 63, 44, 71])\n",
    "e = np.array([ 3.6, 3.9, 2.6, 3.4, 3.8, 3.8, 2.2, 2.1, 2.3, 3.8,\n",
    "               2.2, 2.8, 3.9, 3.1, 3.4, 2.6, 3.4, 3.7, 2.0, 3.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52604697-126c-4e5a-abab-aabd34c7afb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rk-D7T1EGTuK"
   },
   "source": [
    "<span style=\"color:blue\"> <i> 1. Determine $\\theta = [\\theta_0, \\theta_1]$ which maximize the likelihood (or, equivalently, minimize the loss). You can use scipy.optimize.fmin. Plot the best-fit line (on top of data points) using $\\theta$ from the MLE solution. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f317313-8f67-4699-a202-e9b2899ff531",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "executionInfo": {
     "elapsed": 677,
     "status": "ok",
     "timestamp": 1729660192238,
     "user": {
      "displayName": "Henry Liu",
      "userId": "02481607702682765299"
     },
     "user_tz": 420
    },
    "id": "P9ef8IsIGTuK",
    "outputId": "690e1b3e-3d08-4c88-ba6e-801ab952e028",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def squared_loss(theta, x=x, y=y, e=e):\n",
    "    ...\n",
    "\n",
    "theta1 = ...\n",
    "\n",
    "\n",
    "\n",
    "xfit = np.linspace(0, 100)\n",
    "# make your plot here:\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ba39f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa2972-668d-4da5-a860-b225d0ce18e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "D6hTV2l_GTuK"
   },
   "source": [
    "Clearly, we get a poor fit to the data because the squared loss is overly sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88609162-22ee-4f7d-a3a4-c4f77d1cba68",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lBCmAqHCGTuK"
   },
   "source": [
    "The Bayesian approach to accounting for outliers generally involves modifying the model so that the outliers are accounted for. For this data, it is abundantly clear that a simple straight line is not a good fit to our data. So let's propose a more complicated model that has the flexibility to account for outliers. One option is to choose a mixture between a signal and a background:\n",
    "$$p(\\{x_i\\}, \\{y_i\\}, \\{e_i\\} |\\theta, \\{g_i\\}, \\sigma, \\sigma_B) = \\frac{g_i}{\\sqrt{2\\pi e_i^2}} \\exp\\left[-\\frac{(\\hat{y}(x_i|\\theta)-y_i)^2}{2e_i^2}\\right] + \\frac{1-g_i}{\\sqrt{2\\pi \\sigma_B^2}} \\exp\\left[-\\frac{(\\hat{y}(x_i|\\theta)-y_i)^2}{2\\sigma_B^2}\\right]$$\n",
    "What we've done is expanded our model with some nuisance parameters: $\\{g_i\\}$ is a series of weights which range from 0 to 1 and encode for each point $i$ the degree to which it fits the model. $g_i=0$ indicates an outlier, in which case a Gaussian of width $\\sigma_B$ is used in the computation of the likelihood. This $\\sigma_B$ can also be a nuisance parameter, or its value can be set at a sufficiently high number, say 50.\n",
    "\n",
    "Our model is much more complicated now: it has 22 parameters rather than 2, but the majority of these can be considered nuisance parameters, which can be marginalized-out in the end. Let's construct a function which implements this likelihood. As we have not yet covered MCMC sampling in the class, we will implement a negative log likelihood and make use of scipy.optimize.minimize to find the optimal likelihood parameters, taking advantage of the L-BFGS-B bounded optimization algorithm to optimize under our uniform priors.\n",
    "\n",
    "We have clear bounded priors for our parameters: $g_i \\in [0, 1]$ and $\\sigma_B > 0$, while the parameters $\\theta_0$ and $\\theta_1$ are unbounded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc6424-4a6c-433e-a995-897b98efbe70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-iHl6n3HGTuK"
   },
   "source": [
    "<span style=\"color:blue\"> <i> 2. Define the log-likelihood and negative log likelihood functions, for optimization. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999166de-f0f7-475c-92c6-15e3d9b6fc7f",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56911334-9bd4-43f6-91c3-728825c17c52",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x=x, y=y, e=e):\n",
    "    \"\"\"\n",
    "    Due to the nature of scipy.optimize.minimize, we only have one main parameter here: theta. \n",
    "    Here, theta is a 1D array of length len(x) + 3, with theta[0] as the y intercept, theta[1] as the slope\n",
    "    theta[2] as sigma_B and theta[3:] as the g_i parameters. We will be optimizing all these parameters\n",
    "    together.\n",
    "    \"\"\"\n",
    "    \n",
    "    # theta[0] is the y intercept, theta[1] is the slope\n",
    "    sigma_B = theta[2]\n",
    "    g = np.clip(theta[3:], 0, 1)  # g<0 or g>1 leads to NaNs in logarithm\n",
    "\n",
    "    ...\n",
    "\n",
    "def minus_log_likelihood(theta):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805563c7-c918-4fb6-9a00-18d5c16d604f",
   "metadata": {
    "id": "Qabs5i6aGTuL"
   },
   "source": [
    "Now, follow the documentation for defining bounds in scipy.optimize.minimize and define the bounds for the input. You can define bounds as a list of tuples. Note that 'no bound' is defined by using ``None`` instead of a bound value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ddd1ac-5284-4569-b030-67f420fb6833",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "bounds = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9b745-6266-481f-8289-7cf11d89264d",
   "metadata": {},
   "source": [
    "initialize a number of random starting points within the bounds (note that if you initialize only one starting point, you may not find the best-fit values), then do the optimization for each to get the optimal best fit. \n",
    "                                                                                                                                                                                      \n",
    "Your optimal value for x is the                                                                                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0200f-14d2-4822-8241-3ba1aff699f9",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(rng_seed)\n",
    "\n",
    "...\n",
    "\n",
    "# This is the output for your function, minimize this for different initial conditions.\n",
    "opt_fn = ... \n",
    "for i in range(...):\n",
    "    # Start with different initializations of x0 (np.random.uniform with different ranges is recommended)\n",
    "    x0 = ... \n",
    "\n",
    "    # Now do the optimization using the L-BFGS-B method with your defined bounds\n",
    "    opt_p = ...\n",
    "\n",
    "    # Select opt_p if it is the most minimal likelihood. You can use opt_p.fun to get the function results.\n",
    "    opt_p_best = ...\n",
    "\n",
    "# Your optimal values:\n",
    "opt_values = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48135bf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284bb11d-35e4-4235-939d-8e0296906471",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XyXRmPz8GTuL"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "We allowed the model to have a nuisance parameter $0 < g_i < 1$ for each data point: $g_i = 0$ indicates an outlier. We can also allow $\\sigma_B$ to be a nuisance parameter to marginalize over (or just make it a large number). Now, let us define an outlier whenever posterior $E(g_i) < 0.5$.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 3. Using such cutoff at $g = 0.5$, identify an outlier and mark them on the plot. Also, plot the new and old best-fit models over the original data. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fff53a-8968-4cad-b805-4663882bff61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1729660263784,
     "user": {
      "displayName": "Henry Liu",
      "userId": "02481607702682765299"
     },
     "user_tz": 420
    },
    "id": "SGD8Vlo9GTuM",
    "outputId": "1b980586-1e68-4f5b-ab3e-80a20ff18999",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3e59b-fd5d-4bd7-9dba-9a7b3fcf6b9c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oZyk-QKB4gSj"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### Problem 2 - Supernova\n",
    "\n",
    "In this homework, we use a compilation of supernovae data to show that the expansion of the universe is accelerating, and hence it contains dark energy. This is the Nobel prize winning research in 2011 (https://www.nobelprize.org/nobel_prizes/physics/laureates/2011/), and Saul Perlmutter, a professor of physics at Berkeley, shared a prize in 2011 for this discovery.\n",
    "<br><br>\n",
    "\"The expansion history of the universe can be determined quite easily, using as a “standard candle” any distinguishable class of astronomical objects of known intrinsic brightness that can be identified over a wide distance range. As the light from such beacons travels to Earth through an expanding universe, the cosmic expansion stretches not only the distances between galaxy clusters, but also the very wavelengths of the photons en route. By the time the light reaches us, the spectral wavelength $\\lambda$ has thus been redshifted by precisely the same incremental factor $z = \\Delta \\lambda/\\lambda$ by which the cosmos has been stretched in the time interval since the light left its source. The recorded redshift and brightness of each such object thus provide a measurement of the total integrated expansion of the universe since the time the light was emitted. A collection of such measurements, over a sufficient range of distances, would yield an entire historical record of the universe’s expansion.\" (Saul Perlmutter, http://supernova.lbl.gov/PhysicsTodayArticle.pdf).\n",
    "<br><br>\n",
    "Supernovae emerge as extremely promising candidates for measuring the cosmic expansion. Type I Supernovae arises from the collapse of white dwarf stars when the Chandrasekhar limit is reached. Such nuclear chain reaction occurs in the same way and at the same mass, the brightness of these supernovae are always the same. The relationship between the apparent brightness and distance of supernovae depend on the contents and curvature of the universe.\n",
    "<br><br>\n",
    "We can infer the \"luminosity distance\" $D_L$ from measuring the inferred brightness of a supernova of luminosity $L$. Assuming a naive Euclidean approach, if the supernova is observed to have flux $F$, then the area over which the flux is distributed is a sphere radius $D_L$, and hence <br><br>\n",
    "$$F = \\frac{L}{4\\pi D_L^2}.$$\n",
    "<br>\n",
    "In Big Bang cosmology, $D_L$ is given by:\n",
    "<br><br>\n",
    "$$ D_L = \\frac{\\chi(a)}{a} $$\n",
    "<br>\n",
    "where $a$ is the scale factor ($\\frac{\\lambda_0}{\\lambda} = 1 + z = \\frac{a_0}{a}$, and the quantity with the subscript 0 means the value at present. Note that $a_0 = 1, z_0 = 0$.), and $\\chi$ is the comoving distance, the distance between two objects as would be measured instantaneously today. For a photon, $cdt = a(t)d\\chi$, so $\\chi(t) = c\\int_t^{t_0} \\frac{dt'}{a(t')}$. We can write this in terms of a Hubble factor ($H(t) = \\frac{1}{a}\\frac{da}{dt}$), which tells you the expansion rate: $\\chi(a) = c\\int_a^1 \\frac{da'}{a'^2H(a')} = c\\int_0^z \\frac{dz'}{H(z')}$. (change of variable using $a = \\frac{1}{1+z}$.)\n",
    "<br><br>\n",
    "Using the Friedmann equation (which basically solves Einstein's equations for a homogenous and isotropic universe), we can write $H^2$ in terms of the mass density $\\rho$ of the components in the universe: $H^2(z) = H_0^2[\\Omega_m(1+z)^3 + (1-\\Omega_m)(1+z)^2].$ <br><br>\n",
    "$\\Omega$ is the density parameter; it is the ratio of the observed density of matter and energy in the universe ($\\rho$) to the critical density $\\rho_c$ at which the universe would halt is expansion. So $\\Omega_0$ (again, the subscript 0 means the value at the present) is the total mass and energy density of the universe today, and consequently $\\Omega_0 = \\Omega_{m}$ (matter density parameter today; remember we obtained the best-fit value of this parameter in Project 1?) = $\\Omega_{\\mathrm{baryonoic\\ matter}}$ + $\\Omega_{\\mathrm{dark\\ matter}}$. If $\\Omega_0 < 1$, the universe will continue to expand forever. If $\\Omega_0 > 1$, the expansion will stop eventually and the universe will start to recollapse. If $\\Omega_0 = 1$, then the universe is flat and contains enough matter to halt the expansion but not enough to recollapse it. So it will continue expanding, but gradually slowing down all the time, finally running out of steam only in the infinite future. Even including dark matter in this calculation, cosmologists found that all the matters in the universe only amounts to about a quarter of the required critical mass, suggesting a continuously expanding universe with deceleration. Then, using all this, we can write the luminosity distance in terms of the density parameters: <br><br>\n",
    "$$ D_L = \\frac{\\chi(a)}{a} = c(1+z)\\int_0^z \\frac{dz'}{H(z')} = c(1+z)\\int_0^z \\frac{dz'}{H_0[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^2]^{1/2}}  $$ <br>\n",
    "$$ = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^2]^{1/2}}\\ [unit\\ of\\ Mpc] $$\n",
    "<br>\n",
    "where $H_0 = 100\\cdot h\\ [km\\cdot s^{-1} Mpc^{-1}]$.\n",
    "<br><br>\n",
    "Fluxes can be expressed in magnitudes $m$, where $m = -2.5\\cdot\\mathrm{log}_{10}F$ + const. The distance modulus is $\\mu = m - M$ ($M$ is the absolute magnitude, the value of $m$ if the supernova is at a distance 10pc. Then, we have:\n",
    "<br><br>\n",
    "$$ \\mu = 25 + 5\\cdot \\mathrm{log}_{10}\\Big(D_L\\ [in\\ the\\ unit\\ of \\ Mpc]\\Big)$$\n",
    "<br><br>\n",
    "In this assignment, we use the SCP Union2.1 Supernova (SN) Ia compilation. (http://supernova.lbl.gov/union/)\n",
    "<br><br>\n",
    "First, load the measured data: $z$ (redshift), $\\mu$ (distance modulus), $\\sigma(\\mu)$ (error on distance modulus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4051b-dec0-4fbc-a8e3-99581a063d09",
   "metadata": {
    "id": "iOsYQAue4gSk"
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"./sn_z_mu_dmu_plow_union2.1.txt\", usecols=range(1,5))\n",
    "# z\n",
    "z_data = data[:,0]\n",
    "# mu\n",
    "mu_data = data[:,1]\n",
    "# error on mu (sigma(mu))\n",
    "mu_err_data = data[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bd6d3-6ab9-4eb3-b461-2e6696ab26ce",
   "metadata": {
    "id": "N_gNE4W24gSp"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae7136-ebab-490d-84f8-eeff120e9d91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "U350amvl4gSq"
   },
   "source": [
    "Let us assume that the universe is flat (which is a fair assumption since the CMB measurements indicate that the universe has no large-scale curvature). $\\Omega_0 = \\Omega_m + \\Omega_{DE} = 1$. Then, we do not need to worry about the curvature term:<br><br>\n",
    "$$ D_L = \\frac{\\chi(a)}{a} = c(1+z)\\int_0^z \\frac{dz'}{H(z')} = c(1+z)\\int_0^z \\frac{dz'}{H_0[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^{3(1+w)}]^{1/2}}  $$ <br>\n",
    "$$ = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^{3(1+w)}]^{1/2}}\\ [unit\\ of\\ Mpc] $$<br>\n",
    "where $H_0 = 100\\cdot h\\ [km\\cdot s^{-1} Mpc^{-1}]$.<br><br>\n",
    "Assuming that errors are Gaussian (can be justified by averaging over large numbers of SN; central limit theorem), we calculate the likelihood $L$ as: <br><br>\n",
    "$$ L \\propto \\mathrm{exp}\\Big( -\\frac{1}{2} \\sum_{i = 1}^{N_{\\mathrm{SN}}} \\frac{[\\mu_{i,\\ data}(z_i) - \\mu_{i,\\ model}(z_i, \\Omega_m, w)]^2}{\\sigma(\\mu_i)^2} \\Big) $$\n",
    "<br>\n",
    "where $z_i, \\mu_i, \\sigma(\\mu_i)$ are from the measurements, and we compute $\\mu_{model}$ as a function of $z, \\Omega_m, w$.\n",
    "\n",
    "<br><br>\n",
    "First, try the <b> Maximum A Posteriori (MAP) method</b>.\n",
    "<br><br>\n",
    "\n",
    "<span style=\"color:blue\"> <i> 1. Assuming that $h$ = 0.7, find the MAP estimation of $\\Omega_m$ and $w$ (i.e. find $\\Omega_m$ and $w$ which maximizes the likelihood.). As our priors are simple bounds on the parameters ($\\Omega_m \\geq 0$ and $w \\leq 0$), we can use the L-BFGS-B method of bounded optimziation as the previous question)\n",
    "</i></span><br><br>\n",
    "(Hint: This is very similar to the previous problem. Take the log of the likelihood and maximize it using scipy.optimize.minimize (https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) using the L-BFGS-B bounded optimization method as above. The Bounds have been given to you. Note that you need to make initial guesses on the parameters in order to use optimize.minimize . You can set them to be 0. Caveat: \"optimize.minimize\" minimizes a given function, so you should multiply the log-likelihood by $-1$ in order to maximize it using this function.)\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b0de2-35d0-4da3-ab0e-4c4d5c72c442",
   "metadata": {
    "id": "mmKHDHei4gSr",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the terms in the mu model:\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48724d-3a38-433c-8314-18c3e5b57fb6",
   "metadata": {
    "id": "leqWO3tf4gSw",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def minus_log_likelihood(param):\n",
    "    \"Ln of likelihood for flat Universe and SN distance modulus data\"\n",
    "    Omegam, w = param\n",
    "    ...\n",
    "    lnL = ...\n",
    "\n",
    "    return -lnL\n",
    "\n",
    "bounds = [(0, None), # Bound for Omegam: lower bound of 0 and no upper bound\n",
    "          (None, 0)] # Bound for w: no lower bound, upper bound of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdcf829-9cdd-4a68-ab9b-ef9ab8e38e90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2267,
     "status": "ok",
     "timestamp": 1666209181412,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "HDTCi8nt4gS0",
    "outputId": "5bb995c1-1a23-4996-de65-9b995abcb6f9",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# opt_p is the result of your optimize.minimize function\n",
    "opt_p = ...\n",
    "Omegam, w = ...\n",
    "\n",
    "print('MAP solution')\n",
    "print('Omega_m = ', Omegam, ', w = ', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac358c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90107d-6df8-4d6d-9f88-3b1acdd2f31b",
   "metadata": {
    "id": "bkxQzoRg4gS8"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e131d32-e4ed-497d-9157-a8a06a115377",
   "metadata": {
    "id": "BnTqsGtu4gTm"
   },
   "source": [
    "<br><br>\n",
    "Now, include the distance modulus of 12 additional supernovae, which are not-so-good standard candles. They are 3$\\sigma$ away from the best-fit mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e643d56-1d1e-4b6e-b47f-a25e668af779",
   "metadata": {
    "id": "ggGAIJ_S4gTm"
   },
   "outputs": [],
   "source": [
    "# with outliers included\n",
    "data = np.loadtxt(\"./sn_z_mu_dmu_plow_union2.1_outlier.txt\", usecols=range(1,5))\n",
    "# z\n",
    "z_data = data[:,0]\n",
    "# mu|\n",
    "mu_data = data[:,1]\n",
    "# error on mu (sigma(mu))\n",
    "mu_err_data = data[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c0003b-7e53-45cd-a55c-348c801f0368",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CDa5Q7374gTr"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "So we have a total of 592 supernovae, and we can see that the last 12 supernovae seem to be outliers. (i.e. mu_data[580:] contains the distance modulus measurements of these 12 supernovae.)\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 2. Plot the measurements of all 592 supernovae (with errorbar). Show the last 12 supernovae (outliers) with a different color. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfbe983-9028-4d5a-af1b-eb280f2ec615",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1666209397263,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "R7gkVQjT4gTr",
    "outputId": "2080483b-3be7-4f5b-a861-e1fba5ac90bc",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "\n",
    "...\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0.01, 1.5)\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdaaf5-cfd9-4565-b4ab-11c3fbd7907b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1bb89jJ44gTx"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<span style=\"color:blue\"> 3. With 12 outliers included, run the same optimization method as previous to get the new MLE with bounds values on $\\Omega_m$ and $w$. Print your constraints on $w$ and $\\Omega_m$.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ff69d-81ec-452e-ab87-6433f5ca3c8d",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062a52f-bddb-4b25-a136-c9bfc51ccdc6",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# opt_p is the result of your optimize.minimize function\n",
    "opt_p = ...\n",
    "Omegam_q3, w_q3 = ...\n",
    "\n",
    "print('(Outliers Included) MAP solution')\n",
    "print('Omega_m = ', Omegam_q3, ', w = ', w_q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be769f93",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d394a-e567-4d53-9e79-8a48b5d45147",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "h--HMZIQ4gT6"
   },
   "source": [
    "<br><br>\n",
    "Remember that in Problem 1, we used the Gaussian mixture to better model the measurements with outliers. Let us apply the same technique in this case.\n",
    "<br><br>\n",
    "$$ L = \\prod_{i = 1}^{N_{\\mathrm{SN}}} \\Big[ \\frac{g}{\\sqrt{2\\pi\\sigma(\\mu_i)^2}}\\mathrm{exp}\\Big( -\\frac{1}{2} \\frac{[\\mu_{i,\\ data}(z_i) - \\mu_{i,\\ model}(z_i, \\Omega_m, w)]^2}{\\sigma(\\mu_i)^2} \\Big) + \\frac{1-g}{\\sqrt{2\\pi\\sigma_B^2}}\\mathrm{exp}\\Big( -\\frac{1}{2} \\frac{[\\mu_{i,\\ data}(z_i) - \\mu_{i,\\ model}(z_i, \\Omega_m, w) + \\Delta \\mu]^2}{\\sigma_B^2} \\Big) \\Big] $$\n",
    "<br>\n",
    "Here, we have 5 free parameters: $\\Omega_m, w, \\sigma_B, g, \\Delta \\mu$.\n",
    "<br><br>\n",
    "With outliers, we think there is something in the noise we do not really understand, which makes error distribution non-Gaussian. So we hope adding a second Gaussian to the model would better describe the pdf. $g$ determines weights on the two Gaussians. $\\sigma_B^2$ is the variance of the second Gaussian, which we assume to be larger than the variance of the first Gaussian. $\\Delta \\mu$ is the distance modulus offset in the second Gaussian.\n",
    "<br><br>\n",
    "\n",
    "Note the difference between here and our method in Problem 1: Here we only have one $g$ parameter instead of a $g_i$ for each data point. This is because we are not looking for detecting outliers individually in this problem. Rather, we only want to marginalize out the outliers so we only want a generalized distribution for the mixture model rather than assigning each data point to be \"data\" or \"noise\".\n",
    "\n",
    "Use the bounds $\\Omega_m \\geq 0$, $w \\leq 0$, $\\sigma_B \\geq 0$, $0 \\leq g \\leq 1$, and the same L-BFGS-B method as before to find the parameters $\\Omega_0$ and $w$ in the presence of outliers. Note that you may have to experiment with starting point x0.\n",
    "\n",
    "<span style=\"color:blue\"> <i> 4. Use scipy.optimize.minimize to minimize this new model likelihood function.. Print your MAP optimal values on $w$ and $\\Omega_m$, as well as your optimal value on $g$. Note that you may have to experiment with different starting points.  </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca0789-2d01-44ba-ad9c-cf1b0621b724",
   "metadata": {
    "id": "MmNdpb_k4gT7",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46bbf7-ec06-4baf-a48b-027a8018d5cf",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# opt_p is the result of your optimize.minimize function:\n",
    "opt_p = ...\n",
    "Omegam_q4 = ...\n",
    "w_q4 = ...\n",
    "sigma_B = ...\n",
    "g = ...\n",
    "muoffset = ...\n",
    "\n",
    "\n",
    "print('(Outliers Included) MAP solution - Gaussian Mixture Model')\n",
    "print('Omega_m = ', Omegam_q4, ', w = ', w_q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcaff45",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b50238-a2b6-48fb-ae49-5b8b5a4ffccd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "FHEkXlxb4gUD"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<span style=\"color:blue\"> <i> 5. Using the estimates of $\\Omega_m$ and $w$ from your MAP results in Part 3 and Part 4, calculate the distance modulus from theory and plot the curves on top of the measured data. See how they fit. Make sure to highlight the outliers as before. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3df02-4b71-4e2f-84ef-bc0990a1f020",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8vw-PNhJ4gUI",
    "outputId": "8744e8a9-d9fb-49cf-b9ee-73213ff827ab",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c6ea4-22fd-47bd-94eb-57e3825561c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4uEtsHtL4gUN"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15d47a-d39b-4187-84df-f509b8fa3e6c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kHFTmNjd4gUO"
   },
   "source": [
    "Reference: See pg. 8-16 (https://lear.inrialpes.fr/~jegou/bishopreadinggroup/chap9.pdf)\n",
    "<br><br>\n",
    "For this Gaussian mixture model, we wish to maximize the likelihood function with respect to the parameters $g, \\sigma_B, \\Delta \\mu$ for $\\Omega_m = 0.3, w = -1$. In order to do this, we will apply the <b>expectation-maximization (EM)</b> algorithm. This is an iterative method to find maximum likelihood in the case where the model depends on the hidden/latent variable. Here, we call binary variable <b>a</b> as our latent variable such that $p(a_k = 1) = \\pi_k$\n",
    "<br><br>\n",
    "Re-write the likelihood as:\n",
    "<br>\n",
    "$$ L = \\prod_{i = 1}^{N_{\\mathrm{SN}}} \\Big[ \\frac{\\pi_1}{\\sqrt{2\\pi\\sigma(\\mu_i)^2}}\\mathrm{exp}\\Big( -\\frac{1}{2} \\frac{[\\mu_{i,\\ data}(z_i) - \\mu_{i,\\ model}(z_i, \\Omega_m = 0.3, w = -1)]^2}{\\sigma(\\mu_i)^2} \\Big) + \\frac{\\pi_2}{\\sqrt{2\\pi\\sigma_B^2}}\\mathrm{exp}\\Big( -\\frac{1}{2} \\frac{[\\mu_{i,\\ data}(z_i) - \\mu_{i,\\ model}(z_i, \\Omega_m = 0.3, w = -1) - \\mu_{\\mathrm{offset}}]^2}{\\sigma_B^2} \\Big) \\Big] $$\n",
    "<br>\n",
    "$$ = \\prod_{i = 1}^{N_{\\mathrm{SN}}} \\Big[ \\pi_1  \\cdot \\mathrm{Normal}\\big(\\Delta \\mu_i = \\mu_{i,\\ data}-\\mu_{i,\\ model}\\big|\\ \\overline{\\Delta \\mu}_{\\mathrm{class\\ 1}} = 0, \\sigma(\\mu_i)^2 \\big) + \\pi_2 \\cdot \\mathrm{Normal}\\big(\\Delta \\mu_i = \\mu_{i,\\ data}-\\mu_{i,\\ model}\\big|\\ \\overline{\\Delta \\mu}_{\\mathrm{class\\ 2}} = \\mu_{\\mathrm{offset}}, \\sigma_B^2 \\big) \\Big] $$\n",
    "where $\\mu_{i,\\ model}$ assumes $\\Omega_m = 0.3, w = -1$. Suppose that we measure $\\Delta \\mu = \\mu_{i,\\ data}-\\mu_{i,\\ model}$. For the first Gaussian (expected to describe the distribution of 580 non-outlier, standard-candle supernovae), the mean value of $\\Delta \\mu$ is $0$, and its variance is the measurement noise $\\sigma(\\mu)^2$. For the second Gaussian which expects to describe the distribution of 12 outliers, we assume that there will be some offset in $\\mu$ ($\\mu_{\\mathrm{offset}}$), so the mean value of $\\Delta \\mu$ is $\\mu_{\\mathrm{offset}}$, and it has some unknown variance $\\sigma_B^2$.\n",
    "<br><br>\n",
    "Now apply the EM algorithm.\n",
    "<br><br>\n",
    "1. First, initialize: choose $\\pi_1 = 0.95$ and $\\pi_2 = 0.05$. Let $\\mu_{\\mathrm{offset}} = 0, \\sigma_B = 0.5$ initially.\n",
    "<br><br>\n",
    "2. <b>Expectation (E) step</b>: Evaluate the responsibilities using the current parameter values.\n",
    "<br><br>\n",
    "$$ \\gamma_{1,\\ i} = \\frac{\\pi_1  \\cdot \\mathrm{Normal}\\big(\\Delta \\mu_i \\big|\\ \\overline{\\Delta \\mu}_{\\mathrm{class\\ 1}}, \\sigma(\\mu_i)^2 \\big)}{\\pi_1  \\cdot \\mathrm{Normal}\\big(\\Delta \\mu_i \\big|\\ \\overline{\\Delta \\mu}_{\\mathrm{class\\ 1}}, \\sigma(\\mu_i)^2 \\big) + \\pi_2 \\cdot \\mathrm{Normal}\\big(\\Delta \\mu_i \\big|\\ \\overline{\\Delta \\mu}_{\\mathrm{class\\ 2}}, \\sigma_B^2 \\big)} $$\n",
    "$$ \\gamma_{2,\\ i} = \\frac{\\pi_2  \\cdot \\mathrm{Normal}\\big(\\Delta \\mu_i \\big|\\ \\overline{\\Delta \\mu}_{\\mathrm{class\\ 2}}, \\sigma_B^2 \\big)}{\\pi_1  \\cdot \\mathrm{Normal}\\big(\\Delta \\mu_i \\big|\\ \\overline{\\Delta \\mu}_{\\mathrm{class\\ 1}}, \\sigma(\\mu_i)^2 \\big) + \\pi_2 \\cdot \\mathrm{Normal}\\big(\\Delta \\mu_i \\big|\\ \\overline{\\Delta \\mu}_{\\mathrm{class\\ 2}}, \\sigma_B^2 \\big)} $$\n",
    "<br>\n",
    "where $i = 1, ..., N_{SN}$ (number of measurements). Note that $\\gamma_{1}$ and $\\gamma_{2}$ are vectors of length $N_{SN}$. For a supernova $i$, $\\gamma_{1,\\ i}$ describes its probability of belonging to the first class (described by the first Gaussian). (Note: Therefore, in the end, we expect 12 outliers have much higher values of $\\gamma_{2}$ than normal 580 supernovae - i.e. they have much greater probability of belonging to the second class. This is a systematic way to identify an outlier.)\n",
    "<br><br>\n",
    "3. <b>Maximization (M) step</b>: Re-estimate the parameters using the current responsibilities\n",
    "<br><br>\n",
    "The mean $(\\overline{\\Delta \\mu}_{\\mathrm{class\\ 1}} = 0)$ and variance, $\\sigma(\\mu_i)^2$, of the first gaussian are fixed at initial values\n",
    "$$ N_1 = \\sum_{i=1}^{N_{SN}} \\gamma_{1,\\ i},\\ \\ N_2 = \\sum_{i=1}^{N_{SN}} \\gamma_{2,\\ i}$$\n",
    "$$ \\overline{\\Delta \\mu}_{\\mathrm{class\\ 2}} = \\frac{1}{N_2} \\sum_{i=1}^{N_{SN}} \\gamma_{2,\\ i} \\cdot \\Delta \\mu_i  $$\n",
    "$$ \\sigma_B^2 =  \\frac{1}{N_2} \\sum_{i=1}^{N_{SN}} \\gamma_{2,\\ i} \\cdot (\\Delta \\mu_i - \\overline{\\Delta \\mu}_{\\mathrm{class\\ 2}})^2$$\n",
    "$$ \\pi_1 = \\frac{N_1}{{N_{SN}}},\\ \\ \\pi_2 = \\frac{N_2}{{N_{SN}}} $$\n",
    "<br><br>\n",
    "4. Evaluate the log-likelihood and check for convergence of either the parameters or the log likelihood. If the convergence criterion is not satisfied return to step 2.\n",
    "<br><br>\n",
    "\n",
    "<span style=\"color:blue\"> <i> 6. Using EM, calculate the converged values of $\\pi_1$, $\\pi_2$, and $N_2$. $N_2$ is the total number of SN in the second class (can be identified as outliers). Iterate until you reach the convergence (parameters not changing). Then, print out the values of $\\gamma_{2}$ and show that 12 outliers have higher values of $\\gamma_{2}$ than other supernovae. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020ce4d-63fa-4ed9-a1a9-ca68e4e0cfd3",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sBLAuEHx4gUP",
    "outputId": "de3b1883-c8cf-4ed3-d50c-1cfb8f0606e1",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "\n",
    "\n",
    "# Define the values below for autograder use.\n",
    "\n",
    "The following 3 should be floats, defined as the optimized value of Pi1, Pi2 and N2 at the end of iteration.\n",
    "Pi1 = ...\n",
    "Pi2 = ...\n",
    "N2 = ...\n",
    "\n",
    "# The following 2 should be 1D np.arrays, defined as the gamma values for all data points at the final iteration\n",
    "Gamma_n1 = ...\n",
    "Gamma_n2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689194b-f71d-4788-bc53-47ff5c79d35a",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vXCawG0E8IM6",
    "outputId": "10ae829d-24dd-4781-93fc-667833975027",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"The mean value of Gamma_2 for 580 normal supernovae is \", np.mean(Gamma_n2[0:580]))\n",
    "print(\"The mean value of Gamma_2 for 12 outliers is \", np.mean(Gamma_n2[580:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ad406",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c02ed7e-5b4a-4cf2-876b-9373ccce4d85",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LD9boALT4gUi"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Problem 3 - Back to Quasar\n",
    "\n",
    "In HW3, we performed Principal Component Analysis (PCA) on the quasar (QSO) spectra from the Sloan Digital Sky Survey (SDSS); we filtered for high $S/N$ to apply the standard PCA and selected 18 high-$S/N$ spectra of QSOs with redshift 2.0 < z < 2.1, trimmed to $1340 < \\lambda < 1620\\ \\mathring{A}$. Then, using the first three principal eigenvectors from the covariance matrix, we reconstructed each of the 18 QSO spectra.\n",
    "<br><br>\n",
    "In this assignment, we do Expectation Maximization PCA with and without per-observation weights. We use a simple noise fit of PCA components to individual spectra. Finally, using a Gaussian process, we compute the posterior distribution of the QSO's true emission spectrum and sample from it.\n",
    "<br><br>\n",
    "The following analysis is based on https://arxiv.org/pdf/1208.4122.pdf, and https://arxiv.org/pdf/1605.04460.pdf\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c933a-1924-4ea2-a621-9a8e3d2d064f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NAz9msqr4gUj"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "wavelength = np.loadtxt(\"./Problem3_wavelength.txt\")\n",
    "X = np.loadtxt(\"./Problem3_QSOspectra.txt\")\n",
    "ivar = np.loadtxt(\"./Problem3_ivar_flux.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42acf2b-6a5d-4e77-9351-7910007a00ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 2749,
     "status": "ok",
     "timestamp": 1604109283281,
     "user": {
      "displayName": "Byeonghee Yu",
      "photoUrl": "",
      "userId": "05957336094460026679"
     },
     "user_tz": -540
    },
    "id": "WmX34VGH4gUs",
    "outputId": "b62294fa-16e8-4ac0-96c4-4ec20cd88ee1"
   },
   "outputs": [],
   "source": [
    "# Data dimension\n",
    "print( np.shape(wavelength) )\n",
    "print( np.shape(X) )\n",
    "print( np.shape(ivar) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f115bd-3d0a-4635-b1e3-be01b48a84d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "p554LMAH4gU1"
   },
   "source": [
    "In the above cell, we load the following data: wavelength in Angstroms (\"wavelength\"), a 2D array of spectra x fluxes (\"$X$\"), and another 2D array of inverse variances ($1/\\sigma^2$) of the flux array (\"ivar\").\n",
    "<br><br>\n",
    "We have 824 wavelength bins, so \"$X$\" is a 18 $\\times$ 824 matrix, each row containing fluxes of different QSO spectra and each column containing fluxes in different wavelength bins. (e.g. X[i,j] is the measured flux of QSO $i$ in wavelength bin $j$.) Similarly, \"ivar\" is a 18 $\\times$ 824 matrix. (e.g. ivar[i,j] is the inverse variance of the flux of QSO $i$ in wavelength bin $j$.)\n",
    "<br><br>\n",
    "Remember that in HW3, we computed the eigenvectors of the covariance of the quasars, sorted by their descending eigenvalues; we call them the principal components (henceforth denoted by $\\phi$). Suppose that we have $k$ eigenvectors, each of length 824. Construct the matrix of eigenvectors $\\phi = [\\phi_1\\ \\phi_2\\ ...\\ \\phi_k]$, with $\\phi_i$ the $i$th principal eigenvector.<br><br>\n",
    "We can reconstruct the data as:<br><br>\n",
    "$$ \\hat{X} = \\mu + \\sum_k c_k \\phi_k  $$\n",
    "<br>\n",
    "where $\\mu$ is the mean of the initial dataset and $c_k$ is the reconstruction coefficient for eigenvector $\\phi_k$.\n",
    "<br><br>\n",
    "More specifically, we define $\\mu$ as:\n",
    "<br><br>\n",
    "$ \\mu$ $ =\n",
    "    \\begin{bmatrix}\n",
    "        \\overline{x}_1 & \\overline{x}_2 & \\dots  &  \\overline{x}_{824} \\\\\n",
    "    \\end{bmatrix}.$\n",
    "<br><br>\n",
    "The mean-centered data matrix $X_c$ can be defined as:\n",
    "<br><br>\n",
    "$X_c = X - \\mu =\n",
    "    \\begin{bmatrix}\n",
    "        x_{(1,1)} - \\overline{x}_1 & x_{(1,2)} - \\overline{x}_2 & \\dots  & x_{(1,824)} - \\overline{x}_{824} \\\\\n",
    "        x_{(2,1)} - \\overline{x}_1 & x_{(2,2)} - \\overline{x}_2 & \\dots  & x_{(2,824)} - \\overline{x}_{824} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "        x_{(18,1)} - \\overline{x}_1 & x_{(18,2)} - \\overline{x}_2 & \\dots  & x_{(18,824)} - \\overline{x}_{824}\n",
    "    \\end{bmatrix}$\n",
    "<br><br>\n",
    "where $x_{m,n}$ denote the flux of $m$th QSO in $n$th wavelength bin, and $\\overline{x}_k$ is the mean flux in $k$th wavelength bin.\n",
    "<br><br>\n",
    "$ \\mu =\n",
    "    \\begin{bmatrix}\n",
    "        \\overline{x}_1 & \\overline{x}_2 & \\dots  &  \\overline{x}_{824} \\\\\n",
    "    \\end{bmatrix}$\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 1. Plot $\\mu$ as a function of wavelength $\\lambda$.  </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0d909-a616-46f6-a796-e15f75b37bbf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1421,
     "status": "ok",
     "timestamp": 1603214135697,
     "user": {
      "displayName": "Byeonghee Yu",
      "photoUrl": "",
      "userId": "05957336094460026679"
     },
     "user_tz": -540
    },
    "id": "vUJl6Gp_4gU2",
    "outputId": "e89cd921-473c-4a72-c0c4-160d52c22b57",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095c48e-2500-49e0-9219-84ec92075239",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "S_HXdxoe4gVH"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\"Expectation Maximization (EM) is an iterative technique for solving parameters to maximize a likelihood function for models with unknown hidden (or latent) variables. Each iteration involves two steps: finding the expectation value of the hidden variables given the current model (E-step), and then modifying the model parameters to maximize the fit likelihood given the estimates of the hidden variables (M-step).\" (https://arxiv.org/pdf/1208.4122.pdf)\n",
    "<br><br>\n",
    "Now, do Expectation Maximization PCA. In this case, we wish to solve for the eigenvectors, and the latent variables are the coefficients $c$. The likelihood is \"the ability of the eigenvectors to describe the data.\"\n",
    "<br><br>\n",
    "First, find the eigenvector $\\phi_1$ with the highest eigenvalue (the first principal eigenvector):\n",
    "<br><br>\n",
    "1. Initialize: Let $\\phi$ is a random vector of length 824.\n",
    "<br><br>\n",
    "2. <b>E-step</b>: For each QSO $j$, $$c_j = X_{row\\ j} \\cdot \\phi$$ <br> Here, \"$\\cdot$\" represents a dot product, so $X_{row\\ j}$ and $\\phi$ are vectors of length 824, so $c_j$ is a number. $c = [c_1\\ c_2\\ ...\\ c_{18}]$ is a vector of length 18 (because we have 18 QSOs in this problem). So for each QSO $j$, we solve the coefficient $c_j$ which best fits that QSO using $\\phi$.\n",
    "<br><br>\n",
    "3. <b>M-step</b>: $$\\phi = \\frac{\\sum_j c_j\\ X_{row\\ j}}{\\sum_j c_j^2} $$\n",
    "<br>\n",
    "Using the coefficients $c_j$, we update $\\phi$ to find the vector which best fits the data given $c_j$.\n",
    "<br><br>\n",
    "4. Normalize:\n",
    "$$ \\phi = \\frac{\\phi}{|\\phi|} $$\n",
    "<br><br>\n",
    "5. Iterate until converged. Once converged, $c_1 = c$, and $\\phi_1 = \\phi$\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6e7ae-3bc7-4d5e-ba63-9a2ffb173555",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Q5s2T85t4gVI"
   },
   "source": [
    "After you get $\\phi_1$, subtract the projection of $\\phi$ from $X$ ($X - c_1 \\otimes \\phi_1$, where \"$\\otimes$\" is the outer product (https://en.wikipedia.org/wiki/Outer_product). $c_1$ is a vector of length 18, and $\\phi_1$ is a vector of length 824, so $c_1 \\otimes \\phi_1$ is a $18 \\times 824$ matrix.) and repeat the EM algorithm.\n",
    "<br><br>\n",
    "(So to find $\\phi_2$, you should use a data matrix $X - c \\otimes \\phi_1$. To find $\\phi_2$, use $X - c_1 \\otimes \\phi_1 - c_2 \\otimes \\phi_2$), and so on.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 2. Using EM PCA, find the first three principal eigenvectors $\\phi_1, \\phi_2, \\phi_3$ and plot them as a function of wavelength.  </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad283c3-e7da-45c7-9c38-b9de15be582a",
   "metadata": {
    "id": "PiLRsgjW4gVJ",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbde1c9-11aa-4e8a-a520-3d8898ba27e5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "R6aEniG24gVS"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Finally, reconstruct the data using the first principal eigenvectors:\n",
    "<br><br>\n",
    "$$ \\hat{X} = \\sum_{k = 1}^3 c_k \\otimes \\phi_k $$\n",
    "<br>\n",
    "<span style=\"color:blue\"> <i> 3. For any one QSO spectra, plot the original and reconstructed spectra, using the above equation. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b90cb-d4f0-4f17-9acc-908116186f1c",
   "metadata": {
    "id": "dN7TkVKc4gVX",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a4e8e-70f9-412c-a200-47c0b22e381f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8NruM7LW4gVg"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Alternatively, you can also reconstruct the data using \"PC scores.\" (Call the PC score matrix $Z$)\n",
    "<br><br>\n",
    "$$ Z = X_c \\phi $$\n",
    "<br><br>\n",
    "Then, we can reconstruct the data by mapping it back to 824 dimensions with $\\bf \\phi^T$:\n",
    "<br><br>\n",
    "$$ \\hat{X} = \\mu + Z \\phi^T $$\n",
    "<br>\n",
    "<span style=\"color:blue\"> <i> 4. For any one QSO spectra, plot the original and reconstructed spectra, using PC scores. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037acf7-9a88-4703-9990-15be4f078e59",
   "metadata": {
    "id": "WCnwyf8t4gVh",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6df0b8-86d1-4351-a83f-a9775949202c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lNbN7L174gVo"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Now, include noisier QSO spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85d693-5163-4c95-9af6-4f0c454b13f6",
   "metadata": {
    "id": "qX1sV7ZI4gVo"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "wavelength = np.loadtxt(\"./Problem3_wavelength_300.txt\")\n",
    "X = np.loadtxt(\"./Problem3_QSOspectra_300.txt\")\n",
    "ivar = np.loadtxt(\"./Problem3_ivar_flux_300.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f888db0-44ca-4372-8429-bd83481d9ec6",
   "metadata": {
    "id": "IjtjROb74gVw"
   },
   "outputs": [],
   "source": [
    "ivar[ivar==0] = 1.e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff898f-74cd-4239-98d6-6ae83df1bf4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 6880,
     "status": "ok",
     "timestamp": 1603214253615,
     "user": {
      "displayName": "Byeonghee Yu",
      "photoUrl": "",
      "userId": "05957336094460026679"
     },
     "user_tz": -540
    },
    "id": "FrjkRnM84gV4",
    "outputId": "459c009d-427b-43ec-d06e-fc9a094cf4b2"
   },
   "outputs": [],
   "source": [
    "# Data dimension\n",
    "print( np.shape(wavelength) )\n",
    "print( np.shape(X) )\n",
    "print( np.shape(ivar) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb0ad6-1409-43e2-ae4e-2fff15e43258",
   "metadata": {
    "id": "1HKCi6y04gV_"
   },
   "source": [
    "We now have 2562 quasars (including 18 high $S/N$ quasars we had before). The below cell plots the spectra of two quasars; you can see how noisy they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3aa065-f4d8-49f6-99ac-65d181fb90fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 4072,
     "status": "ok",
     "timestamp": 1603214253618,
     "user": {
      "displayName": "Byeonghee Yu",
      "photoUrl": "",
      "userId": "05957336094460026679"
     },
     "user_tz": -540
    },
    "id": "enbOh1ew4gV_",
    "outputId": "3b8613ae-e64b-4a1f-9377-419e0ba75be0"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(15,4))\n",
    "ax = axes[0]; i = 50\n",
    "ax.plot(wavelength, X[i,:])\n",
    "ax.set_xlabel('Wavelength [Angstrom]'); ax.set_ylabel('Flux')\n",
    "\n",
    "ax = axes[1]; i = 500\n",
    "plt.plot(wavelength, X[i,:])\n",
    "ax.set_xlabel('Wavelength [Angstrom]'); ax.set_ylabel('Flux')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca33fb4-6bdf-4c9b-aa17-b2fe5b1f3a1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "P--HZPYU4gWE"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "Now, perform EM PCA on 2562 quasars.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 5. Using EM PCA, find the first 10 principal eigenvectors $\\phi_1, \\phi_2, ..., \\phi_{10}$ and reconstruct the data using them. ($ \\hat{X} = \\sum_{k = 1}^{10} c_k \\otimes \\phi_k $) For any two spectra, plot the original and reconstructed spectra. </i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d980f-a589-48b3-af75-c3fde7251909",
   "metadata": {
    "id": "-ST9OjaN4gWG",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9addca3-9df4-4698-a6c7-a24bde6a1b28",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9yWgfCx84gWL"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "So far we treated all data equally when solving for the eigenvectors. However, we find that some data have considerably larger measurement noise, and they can unduly influence the solution. Now, we perform EM PCA with per-observation weights (called weighted EMPCA) so that the high $S/N$ data receive greater weight. (See https://arxiv.org/pdf/1208.4122.pdf for more detailed explanation. The following description is paraphrased from this paper.)\n",
    "<br><br>\n",
    "Basically, we add weights $w$ to the measured data in M-step: $\\phi = \\sum_j w_j\\ c_j\\ X_{row\\ j}$\n",
    "<br><br>\n",
    "In this case, the situation is more complicated since the measured flux in each wavelength bin for each quasar has a different weight. So we cannot do a simple dot product to derive $c$; instead, we must solve a set of linear equations for $c$. Similarly, M-step must solve a set of linear equations to update $\\phi$ instead of just performing a simple sum. Hence, the weighted EMPCA starts with a set of random orthonormal vectors and iterates over.\n",
    "<br><br>\n",
    "1. Initialize: Let $\\phi$ is a set of random orthonormal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad5bd8-df77-490a-8985-78e15cdee0c7",
   "metadata": {
    "id": "frGE0TQN4gWM"
   },
   "outputs": [],
   "source": [
    "# Create an aray of random orthonormal vectors\n",
    "# Reference: https://github.com/sbailey/empca\n",
    "def _random_orthonormal(nvec, nvar, seed=1):\n",
    "    \"\"\"\n",
    "    Return array of random orthonormal vectors A[nvec, nvar]\n",
    "    Doesn't protect against rare duplicate vectors leading to 0s\n",
    "    \"\"\"\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    A = np.random.normal(size=(nvec, nvar))\n",
    "    for i in range(nvec):\n",
    "        A[i] /= np.linalg.norm(A[i])\n",
    "\n",
    "    for i in range(1, nvec):\n",
    "        for j in range(0, i):\n",
    "            A[i] -= np.dot(A[j], A[i]) * A[j]\n",
    "            A[i] /= np.linalg.norm(A[i])\n",
    "\n",
    "    return A\n",
    "\n",
    "# Number of quasars\n",
    "nQSO = len(X)\n",
    "# Number of wavelength bins\n",
    "nLambda = len(wavelength)\n",
    "# Number of eigenvectors we want\n",
    "nEigvec = 10\n",
    "\n",
    "# A set of random orthonormal vectors\n",
    "phi = _random_orthonormal(nLambda, nEigvec, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624dd1c-016c-4b91-8db4-63b43dd0e459",
   "metadata": {
    "id": "Pa2w-G-E4gWT"
   },
   "source": [
    "2. <b>E-step</b>: $X_{row\\ j} = \\phi\\ c_{col\\ j}$. ($X_{row\\ j}$ refers to $j$th row of $X$, and $c_{col\\ j}$ is $j$th column of $c$. Note that $X$ is a matrix of dimension \"nQSO\" x \"nLambda\", $\\phi$ is a matrix of dimension \"nLambda\" x \"nEigvec\", and $c$ is a matrix of dimension \"nEigvec\" x \"nQSO\".) Solve for $c$ assuming weights $w$.\n",
    "<br><br>\n",
    "We define weight $w$ as the inverse variance (\"ivar\"). (So $w$ is a matrix of dimension \"nQSO\" x \"nLambda\") This makes sense. \"We weight the measured data by the estimated measurement variance so that noisy observations do not unduly affect the solution, while allowing PCA to describe the remaining signal variance.\"\n",
    "<br><br>\n",
    "Now, solve $X_{row\\ j} = \\phi\\ c_{col\\ j}$ for $c_{col\\ j}$ with weights $w_{row\\ j}$. More generally, let $A = \\phi, x = c_{col\\ j}, b = X_{row\\ j}, w = w_{row\\ j}$:\n",
    "<br><br>\n",
    "$$ b = Ax $$<br>\n",
    "$$ wb = wAx$$<br>\n",
    "$$ A^T wb = (A^T w A)x$$<br>\n",
    "$$ (A^T w A)^{-1}A^T wb = x$$<br>\n",
    "<br><br>\n",
    "Hence, we get:<br><br>\n",
    "$$ c_{col\\ j} = (\\phi^T w_{row\\ j}\\ \\phi)^{-1}\\ \\phi^T w_{row\\ j}\\ X_{row\\ j} $$\n",
    "<br><br>\n",
    "In the below cell, we define the function \"_solve.\" <br> _solve(A, b, w) solves $Ax = b$ with weights $w$. This function solves $Ax = b$ with weights $w$ using $x = (A^T w A)^{-1}A^T wb$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9a7d5-153c-4292-b293-2a0e17865034",
   "metadata": {
    "id": "5WK0FSPb4gWU"
   },
   "outputs": [],
   "source": [
    "# Solve Ax = b with weights w using the above set of equations\n",
    "# Reference: https://github.com/sbailey/empca\n",
    "def _solve(A, b, w):\n",
    "    \"\"\"\n",
    "    Solve Ax = b with weights w; return x\n",
    "\n",
    "    A : 2D array\n",
    "    b : 1D array length A.shape[0]\n",
    "    w : 1D array same length as b\n",
    "    \"\"\"\n",
    "\n",
    "    #- Apply weights\n",
    "    # nvar = len(w)\n",
    "    # W = dia_matrix((w, 0), shape=(nvar, nvar))\n",
    "    # bx = A.T.dot( W.dot(b) )\n",
    "    # Ax = A.T.dot( W.dot(A) )\n",
    "\n",
    "    b = A.T.dot( w*b )\n",
    "    A = A.T.dot( (A.T * w).T )\n",
    "\n",
    "    if isinstance(A, scipy.sparse.spmatrix):\n",
    "        x = scipy.sparse.linalg.spsolve(A, b)\n",
    "    else:\n",
    "        x = np.linalg.lstsq(A, b)[0]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467dfd2f-ade0-4e88-81e8-3ba1a6d5973d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bA5alrF54gWa"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Now, in the E-step, for each QSO $j$, we can solve $X_{row\\ j} = \\phi c_{col\\ j}$ for $c_{col\\ j}$ with weights $w_{row\\ j}$ using the function \"_solve\".\n",
    "<br><br>\n",
    "Similarly in the M-step, for each wavelength bin $j$, we can solve $X_{col\\ j} = c^T \\phi_{row\\ j}$ for $\\phi_{row\\ j}$ with weights $w_{col\\ j}$ using the function \"_solve\".\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 6. The below cell uses the weighted EMPCA to find $\\phi$. Fill in the blank and run the weighted EMPCA. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac78b6c-6e6f-4c53-aa60-87767e93fcfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 23608,
     "status": "ok",
     "timestamp": 1603214370426,
     "user": {
      "displayName": "Byeonghee Yu",
      "photoUrl": "",
      "userId": "05957336094460026679"
     },
     "user_tz": -540
    },
    "id": "_q0g020W4gWf",
    "outputId": "257c336c-03e6-4534-c5bd-9047889e3f67",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "C = np.zeros( (nEigvec, nQSO) )\n",
    "\n",
    "W = ivar\n",
    "\n",
    "# Number of iteration for EMPCA\n",
    "niteration = 20\n",
    "\n",
    "for jj in range(niteration):\n",
    "    print(\"iteration\", jj+1, \"/20\")\n",
    "\n",
    "    # E-step\n",
    "    for i in range(nQSO):\n",
    "        b = X[i] # SOLUTIOM\n",
    "        A = ...\n",
    "        w = ...\n",
    "        ... = _solve(A, b, w)\n",
    "        \n",
    "    # M-step\n",
    "    for j in range(nLambda):\n",
    "        b = X[:, j] # SOLUTIOM\n",
    "        A = C.T # SOLUTIOM\n",
    "        w = W[:, j] # SOLUTIOM\n",
    "        ... = _solve(A, b, w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd4a66-35a3-494a-8ffc-21665f74e60b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MZ8A5tiW4gWj"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "Reconstruct the data using $\\phi$:<br><br>\n",
    "$$ \\hat{X} = (\\phi c)^T $$\n",
    "<br><br>\n",
    "$\\phi$ is a matrix of dimension \"nLambda\" x \"nEigvec\", and $c$ is a matrix of dimension \"nEigvec\" x \"nQSO\". So $\\hat{X}$ is a matrix of dimension \"nQSO\" x \"nLambda\" as expected.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 7. Reconstruct the data using the above equation. Remember that you chose two spectra in Part 5. For the same two spectra, plot the original and reconstructed spectra. Part 5 uses EMPCA without weights. Compared to Part 5, does your reconstructed spectra become less noisy?    </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4b58f-b1e1-428d-b26d-2d95d37a42ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "executionInfo": {
     "elapsed": 19460,
     "status": "ok",
     "timestamp": 1603214371502,
     "user": {
      "displayName": "Byeonghee Yu",
      "photoUrl": "",
      "userId": "05957336094460026679"
     },
     "user_tz": -540
    },
    "id": "AkHW0Vhs9II7",
    "outputId": "70a0c4cd-2312-46cb-ab2d-de542ceee8a6",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a20f5b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a8b6e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Due to an issue with the auto-generated PDF on the homework this week, please use Control (or Command) + P to print out the PDF for this week's homework. Submit the zip file to Gradescope still!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247efffb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a37aff",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Physics188-288)",
   "language": "python",
   "name": "shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "p188_288_hw4",
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(theta1[0], 39.69978468408225)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(theta1[1], 0.23621065965464266)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": 6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(opt_p_best.fun, 55.23820660028886, rtol=0.001, atol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(opt_values[0], 31.32052864, atol=0.01)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(opt_values[1], 0.46799489, rtol=0.001, atol=0.01)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.1": {
     "name": "q2.1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(Omegam, 0.2796236523092833, rtol=0.001, atol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(w, -1.0044614428682217, rtol=0.001, atol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.3": {
     "name": "q2.3",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(Omegam_q3, 0.34204016116212227, rtol=0.001, atol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(w_q3, -1.1889642728629435, rtol=0.001, atol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.4": {
     "name": "q2.4",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(Omegam_q4, 0.28, rtol=0.01)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(w_q4, -0.98, rtol=0.1)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.6": {
     "name": "q2.6",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(Pi1, 0.9463, atol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(Pi2, 0.05367, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.mean(Gamma_n2[0:580]) <= 0.06\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.mean(Gamma_n2[580:]) >= 0.7\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
