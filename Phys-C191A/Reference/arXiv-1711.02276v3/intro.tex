\label{sec:intro}

Unlike classical bits, which can be copied ad nauseum, quantum bits --- called qubits --- cannot in general be copied, as a result of the Quantum No-Cloning Theorem.  No-cloning has various negative implications to the handling of quantum information; for example it implies that classical error correction cannot be applied to quantum states, and that it is impossible to transmit a quantum state over a classical channel.  On the flip side, no-cloning has tremendous potential for cryptographic purposes, where the adversary is prevented from various strategies that involve copying.  For example, Wiesner~\cite{Wiesner83} shows that if a quantum state is used as a banknote, no-cloning means that an adversary cannot duplicate the note.  This is clearly impossible with classical bits.  Wiesner's idea can also be seen as the starting point for quantum key distribution~\cite{BenBra84}, which can be used to securely exchange keys over a public channel, even against computationally unbounded eavesdropping adversaries.

In this work, we investigate no-cloning in the presence of computationally bounded adversaries, and it's implications to cryptography.  To motivate this discussion, consider the following two important applications:
\begin{itemize}
	\item A public key quantum money scheme allows anyone to verify banknotes.  This remedies a key limitation of Wiesner's scheme, which requires sending the banknote back to the mint for verification.  The mint has a \emph{secret} classical description of the banknote which it can use to verify; if this description is made public, then the scheme is completely broken.  Requiring the mint for verification represents an obvious logistical hurdle.  In contrast, a public key quantum money scheme can be verified locally without the mint's involvement.  Yet, even with the ability to verify a banknote, it is impossible for anyone (save the mint) to create new notes.
	\item Many cryptographic settings such as multiparty computation require a random string to be created by a trusted party during a set up phase.  But what if the randomness creator is not trusted?  One would still hope for some way to verify that the strings it produces are still random, or at least have some amount of (min-)entropy.  At a minimum, one would hope for a guarantee that their string is different from any previous or future string that will be generated for anyone else.  Classically, these goals are impossible.  But quantumly, one may hope to create proofs that are unclonable, so that only a single user can possibly ever receive a valid proof for a particular string.
\end{itemize}

Notice that in both settings above, a computationally unbounded adversary can always break the scheme.  For public key quantum money, the following attack produces a valid banknote from scratch in exponential-time: generate a random candidate quantum money state and apply the verification procedure.  If it accepts, output the state; otherwise try again.  Similarly, in the verifiable randomness setting, an exponential-time adversary can always run the randomness generating procedure until it gets two copies of the same random string, along with two valid proofs for that string.  Then it can give the same string (but different valid proofs) to two different users.  With the current state of knowledge of complexity theory, achieving security against a computationally bounded adversary means computational assumptions are required; in particular, both scenarios imply at a minimum one-way functions secure against quantum adversaries.  

\medskip

Combining no-cloning with computational assumptions is a subtle task.  A typical computational assumption posits that a \emph{classical} problem (that is, the inputs and outputs of the problem are classical) is computationally hard, such as solving general NP problems or finding short vectors in lattices.  In the quantum regime, we ask that the classical problem is hard \emph{even for quantum computers.}  However, it is still desirable for the assumption to involve a classical problem, rather than a quantum problem (quantum states as inputs and outputs).  This is for several reasons.  For one, we want a violation of the assumption to lead to a mathematically interesting result, and this seems much more likely for classical problems.  Furthermore, it is much harder for the research community to study and analyze a quantum assumption, since it will be hard to isolate the features of the problem that make it hard.  Therefore, an important goal in quantum cryptography is 

\begin{center}{\it Combine no-cloning and computational assumptions about\\ classical problems to obtain no-cloning-with-verification.}
\end{center}

In addition to the underlying assumption being classical, it would ideally also be one that has been previously studied by cryptographers, and ideally used in other cryptographic contexts.  This would give the strongest possible evidence that the assumption, and hence application, are secure.

\medskip

For now, we focus on the setting of public key quantum money.  Constructing such quantum money from a classical hardness assumption is a surprisingly difficult task.  One barrier is the following.  Security would be proved by reduction, and algorithm that interacts with a supposed quantum money adversary and acts as an adversary for the the underlying \emph{classical} computational assumption.  Note that the adversary expects as input a valid banknote, which the reduction must supply.  Then it appears the reduction should somehow use the adversary's forgery to break the computational assumption.  But if the reduction can generate a single valid banknote, there is nothing preventing it from generating a second --- recall that the underlying assumption is classical, so we cannot rely on the assumption to provide us with an un-clonable state.  Therefore, if the reduction works, it would appear that the reduction can create two banknotes for itself, in which case it can break the underlying assumption without the aid of the adversary.  This would imply that the underlying assumption is in fact false.

The above difficulties become even more apparent when considering the known public key quantum money schemes.  The first proposed scheme by Aaronson~\cite{CCC:Aaronson09} had no security proof, and was subsequently broken by Lutomirski et al.~\cite{ITCS:LAFGKH10}.  The next proposed scheme by Farhi et al.~\cite{ITCS:FGHLS12} also has no security proof, though this scheme still remains unbroken.  However, the scheme is complicated, and it is unclear which quantum states are accepted by the verification procedure; it might be that there are dishonest banknotes that are both easy to construct, but are still accepted by the verification procedure.

Finally, the third candidate by Aaronson and Christiano~\cite{STOC:AarChr12} actually \emph{does} prove security using a classical computational problem.  However, in order to circumvent the barrier discussed above, the classical problem has a highly non-standard format.  They observe that a polynomial-time algorithm can, by random guessing, produce a valid banknote with some exponentially-small probability $p$, while random guessing can only produce two valid banknotes with probability $p^2$.  Therefore, their reduction first generates a valid banknote with probability $p$, runs the adversary on the banknote, and then uses the adversary's forgery to increase its success probability for some task.  This reduction strategy requires a very carefully crafted assumption, where it is assumed hard to solve a particular problem in polynomial time with exponentially-small probability $p$, even though it can easily be solved with probability $p^2$.  

In contrast, typical assumptions in cryptography involve polynomial-time algorithms and \emph{inverse-polynomial} success probabilities, rather than exponential.  (Sub)exponential hardness assumptions are sometimes made, but even then the assumptions are usually closed under polynomial changes in adversary running times or success probabilities, and therefore make no distinction between $p$ and $p^2$.  In addition to the flavor of assumption being highly non-standard, Aaronson and Christiano's assumption --- as well as their scheme --- have been subsequently broken~\cite{PKC:PenFauPer15,Aaronson16}. 

\medskip

Turning to the verifiable randomness setting, things appear even more difficult.  Indeed, our requirements for verifiable randomness imply an even stronger version of computational no-cloning: an adversary should not be able to copy a state, even if it can verify the state, and \emph{even if it devised the original state itself}.  Indeed, without such a restriction, an adversary may be able to come up with a dishonest proof of randomness, perhaps by deviating from the proper proof generating procedure, that it \emph{can} clone arbitrarily many times.  Therefore, a fascinating objective is to

\begin{center}{\it Obtain a no-cloning theorem, even for settings where the adversary\\ controls the entire process for generating the original state.}
\end{center}

\subsection{This Work: Strong Variants of No-Cloning}

In this work, we study strong variants of quantum no-cloning, in particular public key quantum money, and uncover relationships between no-cloning and various cryptographic applications.

\subsubsection{Quantum Lightning Never Strikes the Same State Twice}

The old adage about lightning is of course false, but the idea nonetheless captures some of the features we would like for the verified randomness setting discussed above.  Suppose a magical randomness generator could go out into a thunderstorm, and ``freeze'' and ``capture'' lightning bolts as they strike.  Every lightning bolt will be different.  The randomness generator then somehow extracts a fingerprint or serial number from the frozen lightning bolt (say, hashing the image of the bolt from a particular direction).  The serial number will serve as the random string, and the frozen lightning bolt will be the proof of randomness; since every bolt is different, this ensures that the bolts, and hence serial numbers, have some amount of entropy.

Of course, it may be that there are other ways to create lightning other than walking out into a thunderstorm (Tesla coils come to mind).  We therefore would like that, no matter how the lightning is generated, be it from thunderstorms or in a carefully controlled laboratory environment, every bolt has a unique fingerprint/serial number. 

We seek a complexity-theoretic version of this magical frozen lightning object, namely a phenomenon which guarantees different outcomes every time, no matter how the phenomenon is generated.  We will necessarily rely on quantum no-cloning --- since in principle a classical phenomenon can be replicated by starting with the same initial conditions --- and hence we call our notion \emph{quantum lightning}.  Quantum lightning, roughly, is a strengthening of public key quantum money where the procedure to generate new banknotes itself is public, allowing anyone to generate banknotes.  Nevertheless, it is impossible for an adversary to construct two notes with the same serial number.  This is a surprising and counter-intuitive property, as the adversary knows how to generate banknotes, and moreover has full control over how it does so; in particular it can deviate from the generation procedure any way it wants, as long as it is computationally efficient.  Nonetheless, it cannot devise a malicious note generation procedure that allows it to construct the same note twice.  This concept of quantum money can be seen as a formalization of the concept of ``collision-free'' public key quantum money due to Lutomirski et al.~\cite{ITCS:LAFGKH10}.  

\medskip


Slightly more precisely, a quantum lightning protocol consists of two efficient (quantum) algorithms.  The first is a bolt generation procedure, or storm, $\genbolt$, which generates a quantum state $\qlightning$ on each invocation.  The second algorithm, $\verbolt$, meanwhile verifies bolts as valid and also extracts a fingerprint/serial number of the bolt.  For correctness, we require that (1) $\verbolt$ always accepts bolts produced by $\genbolt$, (2) it does not perturb valid bolts, and (3) that it will always output the same serial number on a given bolt.

For security, we require the following: it is computationally infeasible to produce two bolts $\qlightning[0]$ and $\qlightning[1]$ such that $\verbolt$ accepts both and outputs identical serial numbers.  This is true for even for \emph{adversarial} storms $\advlightning$, even those that depart from $\genbolt$ or produce entangled bolts, so long as $\advlightning$ is computationally efficient.

\paragraph{Applications.} Quantum lightning as described has several interesting applications:
\begin{itemize}
	\item {\bf Quantum money.}  Quantum lightning easily gives quantum money.  A banknote is just a bolt, with the associated serial number signed by the bank using an arbitrary classical signature scheme.  Any banknote forgery must either forge the bank's signature, or must produce two bolts with the same serial number, violating quantum lightning security.
	\item {\bf Verifiable min-entropy}.  Quantum lightning also gives a way to generate random strings along with a proof that the string is random, or at least has min-entropy.  To see this, consider an adversarial bolt generation procedure that produces bolts such that the associated serial number has low min-entropy.  Then by running this procedure several times, one will eventually obtain in polynomial time two bolts with the same serial number, violating security.
	
	Therefore, to generate a verifiable random string, generate a new bolt using $\genbolt$.  The string is the bolt's serial number, and $\genbolt$ serves as a proof of min-entropy, which is verified using $\verbolt$.
	
	\item {\bf Decentralized Currency.} Finally, quantum lightning yields a simple new construction of totally decentralized digital currency.  Coins are just bolts, except the serial number must hash to a string that begins with a certain number of 0's.  Anyone can produce coins by generating bolts until the hash begins with enough 0's.  Moreover, verification is just $\verbolt$, and does not require any interaction or coordination with other users of the system.  This is an advantage over classical cryptocurrencies such as BitCoin, which require a large public and dynamic ledger, and requires a pool of miners to verify transactions.  Our protocol does have significant limitations relative to classical cryptocurrencies, which likely make it only a toy object.  We hope that further developments will yield a scheme that overcomes these limitations.
\end{itemize}

\subsubsection{Connections to Post-quantum Security}

One simple folklore way to construct a state that can only be constructed once but never a second time is to use a collision-resistant hash function $H$.  First, generate a uniform superposition of inputs.  Then apply the $H$ in superposition, and measure the result $y$.  The state collapses to the superposition $|\psi_y\rangle$ of all pre-images $x$ of $y$.  

Notice that, while it is easy to sample states $|\psi_y\rangle$, it is impossible to sample two copies of the same $|\psi_y\rangle$.  Indeed, given two copies of $|\psi_y\rangle$, simply measure both copies.  Since these are superpositions over many inputs, each state will likely yield a different $x$.  The two $x$'s obtained are both pre-images of the same $y$, and therefore constitute a collision for $H$.

The above idea does not yet yield quantum lightning.  For verification, one can hash the state to get the serial number $y$, but this alone is insufficient.  For example, an adversarial storm can simply choose a random string $x$, and output $|x\rangle$ twice as its two copies of the same state.  Of course, $|x\rangle$ is not equal to $|\psi_y\rangle$ for any $y$.  However, the verification procedure just described does not distinguish between these two states.  

What one needs therefore is mechanism to distinguish a random $|x\rangle$ from a random $|\psi_y\rangle$.  Interestingly, as observed by Unruh~\cite{EC:Unruh16}, this is exactly \emph{the opposite} what one would normally want from a hash function.  Consider the usual way of building a computationally binding commitment from a collision resistant hash function: to commit to a message $m$, choose a random $r$ and output $H(m,r)$.  Classically, this is computationally binding by the collision resistance of $H$: if an adversary can open the commitment to two different values, this immediately yields a collision for $H$.  Unruh~\cite{EC:Unruh16} shows in the quantum setting, collision resistance --- even against quantum adversaries --- is not enough.  Indeed, he shows that for certain hash functions $H$ it may be possible for the adversary to produce a commitment, and only afterward decide on the committed value.  Essentially, the adversary constructs a superposition of pre-images $|\psi_y\rangle$ as above, and then uses particular properties of $H$ to perturb $|\psi_y\rangle$ so that it becomes a different superposition of pre-images of $y$.   Then one simply de-commits to any message by first modifying the superposition and then measuring.  This does not violate the collision-resistance of $H$: since the adversary cannot copy $|\psi_y\rangle$, the adversary can only ever perform this procedure once and obtain only a single de-commitment.  

To overcome this potential limitation, Unruh defines a notion of \emph{collapsing} hash functions.  Roughly, these are hash functions for which $|x\rangle$ and $|\psi_y\rangle$ are \emph{indistinguishable}.  Using such hash functions to build commitments, one obtains \emph{collapse-binding} commitments, for which the attack above is impossible.  Finally, he shows that a random oracle is collapse binding.

More generally, an implicit assumption in many classical settings is that, if an adversary can modify one value into another, then it can produce both the original and modified value simultaneously.  For example, in a commitment scheme, if a classical adversary can de-commit to both 0 or 1, it can then also simultaneously de-commit to both 0 and 1 by first de-committing to 0, and then re-winding and de-committing to 1.  Thus it is natural classically to require that it is impossible to simultaneously produce de-commitments to both 0 and 1.  Similarly, for signatures, if an adversary can modify a signed message $m_0$ into a signed message $m_1$, then it can simultaneously produce two signed messages $m_0,m_1$.  This inspires the Boneh-Zhandry~\cite{EC:BonZha13,C:BonZha13} definition of security for signatures in the presence of quantum adversaries, which says that after seeing a (superposition of) signed messages, the adversary cannot produce two signed messages.

However, a true quantum adversary may be able, for some schemes, to set things up so that it can modify a (superposition) of values into one of many possibilities, but still only be able to ever produce a single value.  For example, it many be that an adversary sees a superposition of signed messages that always begin with 0, but somehow modifies the superposition to obtain a signed message that begins with a 1.  This limitation for signatures was observed by Garg, Yuen, and Zhandry~\cite{C:GarYueZha17}, who then give a much stronger notion to fix this issue\footnote{Garg et al. only actually discuss message authentication codes, but the same idea applies to signatures}.

\medskip

Inspired by the above observations, we formulate a series of win-win results for quantum lightning and quantum money.  In particular, we show, roughly,

\begin{theorem}[informal] If $H$ is a hash function that is collision resistant against quantum adversaries, then either (1) $H$ is collapsing or (2) it can be used to build quantum lightning without any additional computational assumptions.\footnote{Technically, there is a slight gap due to the difference between \emph{non-negligible} and \emph{inverse polynomial}.  Essentially what we show is that the theorem holds for fixed values of the security parameter, but whether (1) or (2) happens may vary across different security parameters.}
\end{theorem}
The construction of quantum lightning is inspired by the outline above.  One difficulty is that above we needed a perfect distinguisher, whereas a collapsing adversary may only have a non-negligible advantage.  To obtain an actual quantum lightning scheme, we need to repeat the scheme in parallel many times to boost the distinguish advantage to essentially perfect.  Still, defining verification so that we can prove security is a non-trivial task.  Indeed, while it is possible to define a verification procedure that accepts valid bolts, it is much harder to analyze what sorts of invalid bolts might be accepted by the verification procedure.  For example, it may be that a certain sparse distribution on $|x\rangle$ is actually accepted by the verification procedure, even though a random $|x\rangle$ is almost always rejected.  Using a careful argument, we show nonetheless how to verify and prove security.  We also show that

\begin{theorem}[informal] Any \emph{non-interactive} commitment scheme that is computationally binding against quantum adversaries is either collapse-binding, or it can be used to build quantum lightning without any additional computational assumptions.
\end{theorem}

The above theorem crucially relies on the commitment scheme being non-interactive: the serial number of the bolt is the sender's single message, along with his private quantum state.  If the commitment scheme is not collapse-binding, the sender's private state can be verified to be in superposition.  If a adversary produces two identical bolts, these bolts can be measured to obtain two openings, violating computational binding.  In contrast, in the case of interactive commitments, the bolt should be expanded to the transcript of the interaction between the sender and receiver.  Unfortunately, for quantum lightning security, the transcript is generated by an adversary, who can deviate from the honest receiver's protocol.  Since the commitment scheme is only binding when the receiver is run honestly, we cannot prove security in this setting.

Instead, we consider the weaker goal of constructing public key quantum money.  Here, since the mint produces bolts, the original bolt is honestly generated.  The mint then signs the transcript using a standard signature scheme (which can be built from one-way functions, and hence implied by commitments).  If the adversary duplicates this banknote, it is duplicating an honest commitment transcript, but the note can be measured to obtain two different openings, breaking computational binding.  This gives us the following:

\begin{theorem}[informal] Any interactive commitment scheme that is computationally binding against quantum adversaries is either collapse-binding, or it can be used to build public key quantum money without any additional computational assumptions.
\end{theorem}

\noindent Finally, we extend these ideas to a win-win result for quantum money and digital signatures:

\begin{theorem}[informal] Any one-time signature scheme that is Boneh-Zhandry secure is either Garg-Yuen-Zhandry secure, or it can be used to build public key quantum money without any additional computational assumptions.
\end{theorem}

Given the difficulty of constructing public key quantum money (let alone quantum lightning), the above results suggest that most natural constructions of collision resistant hash functions are likely already collapsing, with analogous statements for commitment schemes and signatures.  If they surprisingly turn out to not meet the stronger quantum notions, then we would immediately obtain a construction of public key quantum money from simple tools.

\medskip

Notice that using our win-win results give a potential route toward proving the security of quantum money/lightning in a way that avoids the barrier discussed above.  Consider building quantum money from quantum lightning, and in turn building quantum lightning from a collision-resistant non-collapsing hash function.  Recall that a banknote is a bolt, together with the mint's signature on the bolt's serial number.  A quantum money adversary either (1) duplicates a bolt to yield two bolts with the same serial number (and hence same signature), or (2) produces a second bolt with a different serial number, as well as a forged signature on that serial number.  Notice that (2) is impossible simply by the unforgeability of the mint's signature.  Meanwhile, in proving that (1) is impossible, our reduction actually \emph{can} produce arbitrary quantum money states (for this step, we assume the reduction is given the signing key).  The key is that the reduction on its own \emph{cannot} produce the same quantum money state twice, but it \emph{can} do so using the adversary's cloning abilities, allowing it to break the underlying hard problem.



\subsubsection{Constructing Quantum Lightning}


We now turn to actually building quantum lightning, and hence quantum money.  Following our win-win results, we would like a \emph{non-collapsing} collision-resistant hash function.  Unfortunately, Unruh's counterexample does not yield an explicit construction.  Instead, he builds on techniques of~\cite{FOCS:AmbRosUnr14} to give a hash function relative to a \emph{quantum} oracle\footnote{that is, the oracle itself performs quantum operations}.  As it is currently unknown how to obfuscate quantum oracles with a meaningful notion of security, this does not give even a candidate construction of quantum lightning.  Instead, we focus on specific standard-model constructions of hash functions.  Finding suitable hash functions is surprisingly challenging; we were only able to find a single candidate, and leave finding additional candidates as a challenging open problem.

Our construction is based on low-degree hash functions, which were previously studied by Ding and Yang~\cite{DingYang08} and Applebaum et al.~\cite{ITCS:AHIKV17} with the goal of constructing very efficient hash functions.  Specifically, we will use hash functions defined by a random set of degree-2 polynomials over $\F_2$.  That is, the hash function is specified by a set of polynomials $P_1,\dots,P_n$ in $m$ variables over $\F_2$, where each $P_i$ is of degree 2.  The output of $H(x)$ is just the set of $i$ outputs $P_i(x)$.  

As we will see below, such hash functions are not collision resistant, and therefore are not immediately applicable to our win-win results.  Nonetheless, these hash functions will serve as a useful starting point.  In particular, we show that if $m\approx n^2$, then $H$ is not collapsing.  That is, it is possible to distinguish $|\psi_y\rangle$ (the uniform superposition over pre-images of $y$) from $|x\rangle$, for a random $x,y$.  If the $P_i$'s were linear even for $m=\Omega(n)$, this would be trivial by taking the quantum Fourier transform (which over $\F_2$ is just the Hadamard gate).  Since the $P_i$'s have degree 2, our distinguisher is more complicated.  Instead, we observe that if we perform the Hadamard gate to just one input qubit and measure, the result is a derivative of some unknown linear combination of the $P_i$s; the derivative therefore effectively reduces the problem to a linear one, which we can solve more easily.  From this measurement, we obtain a single linear equation in $n$ unknowns, whose coefficients are determined by the remaining $m-1$ inputs bits.  

We cannot simply repeat the process to get more linear equations, since the Hadamard gate introduced a phase term.  Moreover, if we record the coefficients of the linear equation, this amounts to a measurement on the remaining $m-1$ inputs bits, further perturbing the state.  Nevertheless, we show how to correct the phase term and the measurement by performing a linear transformation to the $m-1$ remaining inputs.  This process consumes $n$ inputs bits, resulting in a system on $m-n-1$ qubits.  However, now the state is ready to generate another linear equation in the same $n$ unknowns.  We then repeat approximately $n$ times, and the resulting system of equations has a solution if and only if the original state was $|\psi_y\rangle$ for some $y$.  Overall, this distinguisher requires $m\approx n^2$ input bits.  

Unfortunately, degree-2 hash functions are not collision resistant, as shown by Ding and Yang~\cite{DingYang08} and Applebaum et al.~\cite{ITCS:AHIKV17}.  This is because the equations $P_i(x_0)=P_i(x_1)$ can be linearized and then solved using linear algebra.  This attack exists even if $m=n+1$, and therefore certainly applies to our setting where $m\approx n^2$.  Moreover, we show how to extend the attack so that if $m\approx rn$, it is possible to construct a dimension-$r$ affine space of colliding inputs, thus giving $2^r$ colliding inputs.  If we insist on the colliding inputs having no affine relations, then we show that it is still possible to construct $r+1$ colliding inputs.  This means that, for our non-collapsing hash function above, not only is it non-collision resistant, but it is possible to find many colliding inputs.  Even worse, we can use our attack to generate $r+1$ identical copies of $|\psi_y\rangle$ for the same $y$, meaning our scheme so far is certainly not a quantum lightning scheme.

\smallskip

Despite the above attacks, Applebaum et al. conjecture that random degree-2 polynomials are still one-way, and even second-pre-image resistant.  This is because the above attacks crucially have little control over the output $y$ or the particular colliding inputs that are generated.  We conjecture moreover that it is impossible to devise $2(r+1)$ colliding inputs that have no affine relations.  This conjecture seems plausible in light of known attacks; note that the best-known attacks cannot even generate $r+2$ non-affine colliding inputs.  

Using this conjecture, we obtain a quantum lightning scheme as follows.  Our bolt is $\qlightning=|\psi_y\rangle^{\otimes (r+1)}$ for some $y$, which we generate using the attack above.  For verification, if we set $r\approx n$, we can use our distinguisher on each copy of $|\psi_y\rangle$ to verify that it has the proper form.  We also verify that each of the $r$ states have the same hash $y$ under $H$.  The serial number for $\qlightning$ is $y$.  

If an adversary constructs two states $\qlightning[0],\qlightning[1]$ which both pass verification and have the same serial number $y$, we show that the joint state \emph{after} verification must be precisely $|\psi_y\rangle^{\otimes 2(r+1)}$.  By measuring this state, we obtain $2(r+1)$ random pre-images.  With overwhelming probability, these colliding inputs will have no affine relationships.  Therefore, any quantum lightning adversary violates our conjecture.

\medskip

We note that our quantum lightning scheme is not an instance of a collision-resistant non-collapsing hash function.  Nonetheless, we show that it can be modified to obtain such a function.  Security will still be based on the same exact computational assumption, though the derived hash function will no longer be a degree-2 hash function.

Thus we obtain the first construction of quantum lightning, and hence public key quantum money, based on a plausible conjecture about the quantum hardness of a \emph{classical} computational problem.  Moreover our problem has to some extent been previously studied in the literature.  We also give the first plausible \emph{standard model} construction of a hash function that is not collapsing.  In the process, we establish a general approach to constructing quantum lightning/money, by constructing collision resistant hash functions that are not collapsing.  We leave as an interesting open question whether such hash functions can be constructed from more mainstream assumptions, such as the hardness of lattice problems or on generic assumptions.  



\subsection{Quantum Money From Obfuscation}

Finally, we consider the simpler task of constructing public key quantum money.  One possibility is based on Aaronson and Christiano's broken scheme~\cite{STOC:AarChr12}.  In their scheme, a quantum banknote $|\$\rangle$ is a uniform superposition over some subspace $S$, that is known only to the bank.  The quantum Fourier transform of such a state is the uniform superposition over the dual subspace $S^\bot$.  This gives a simple way to check the banknote: test if $|\$\rangle$ lies in $S$, and whether it's Fourier transform lies in $S^\bot$.  Aaronson and Christiano show that the only state which can pass verification is $|\$\rangle$.  

To make this scheme public key, one gives out a mechanism to test for membership in $S$ and $S^\bot$, without actually revealing $S,S^\bot$.  This essentially means obfuscating the functions that decide membership.  Aaronson and Christiano's scheme can be seen as a candidate obfuscator for subspaces.  While unfortunately their obfuscator has since been broken, one may hope to instantiate their scheme using recent advances in general-purpose program obfuscation, specifically indistinguishability obfuscation (iO)~\cite{C:BGIRSVY01,FOCS:GGHRSW13}.  

On the positive side, Aaronson and Christiano show that their scheme is secure if the subspaces are provided as quantum-accessible black boxes, giving hope that some obfuscation of the subspaces will work.  Unfortunately, proving security relative to iO appears a difficult task.  One limitation is the barrier discussed above, that any reduction must be able to produce a valid banknote, which means it can also produce two banknotes.  Yet at the same time, it somehow has to use the adversary's forgery (a second banknote) to break the iO scheme.  Note that this situation is different from the quantum lightning setting, where there were many valid states, and no process could generate the same state twice.  Here, there is a single valid state (the state $|\$\rangle$), and it would appear the reduction must be able to construct this precise state exactly once, but not twice.  Such a reduction would clearly be impossible.  As discussed above Aaronson and Christiano circumvent this issue by using a non-standard type of assumption; their technique is not relevant for standard definitions of iO.

\medskip

Our solution is to separate the proof into two phases.  In the first, we change the spaces obfuscated from $S,S^\bot$ to $T_0,T_1$, where $T_0$ is a random unknown subspace containing $S$, and $T_1$ is a unknown random subspace containing $S^\bot$.  This modification can be proved undetectable using a weak form of obfuscation we define, called subspace-hiding obfuscation, which in turn is implied by iO.  Note that in this step, we even allow the reduction to know $S$ (but not $T_0,T_1$), so it can produce as many copies of $|\$\rangle$ as it would like to feed to the adversary.  The reduction does not care about the adversary's forgery directly, only whether or not the adversary successfully forges.  If the adversary forges when given obfuscations of $S,S^\bot$, it must also forge under $T_0,T_1$, else it can distinguish the two cases and hence break the obfuscation.  By using the adversary in this way, we avoid the apparent difficulties above.

In the next step, we notice that, conditioned on $T_0,T_1$, the space $S$ is a random subspace between $T_1^\bot$ and $T_0$.  Thus conditioned on $T_0,T_1$, the adversary clones a state $|\$\rangle$ defined by a random subspace $S$ between $T_1^\bot$ and $T_0$.  The number of possible $S$ is much larger than the dimension of the state $|\$\rangle$, so in particular the states cannot be orthogonal.  Thus, by no-cloning, duplication is impossible.  We need to be careful however, since we want to rule out adversaries that forge with even very low success probabilities.  To do so, we need to precisely quantify the no-cloning theorem, which we do.  We believe our new no-cloning theorem may be of independent interest.  We note that when applying no-cloning, we do not rely on the secrecy of $T_0,T_1$, but only that $S$ is hidden.  Intuitively, there are exponentially many more $S$'s between $T_0,T_1$ than the dimension of the space $|\$\rangle$ belongs to, so no-cloning implies that a forger has negligible success probability. Thus we reach a contradiction, showing that the original adversary could not exist.

\medskip

We also show how to view Aaronson and Christiano's scheme as a signature scheme; we show that the signature scheme satisfies the Boneh-Zhandry definition, but not the strong Garg-Yuen-Zhandry notion.  Thus, we can view Aaronson and Christiano's scheme as an instance of our win-win results, and moreover provide the first separation between the two security notions for signatures.

We note that our result potentially relies on a much weaker notion of obfuscation that full iO, giving hope that security can be based on weaker assumptions.  For example, an intriguing open question is whether or not recent constructions of obfuscation for certain evasive functions~\cite{EPRINT:WicZir17,EPRINT:GoyKopWat17} based on LWE can be used to instantiate our notion of subspace hiding obfuscation.  This gives another route toward building quantum money from hard lattice problems.  This is particularly important at the present time, where new quantum attacks have called into question the security of full-fledged iO in the quantum setting (see below for a discussion).  

Finally, we note that independently of whether iO exists in the quantum setting, black box oracles do provide subspace hiding, a fact which can be based easily on the quantum lower bounds for unstructured search~\cite{BBBV97}.  With this insight, our proof strategy can be used to give a simplified analysis of Aaronson and Christiano's black-box scheme.  Their proof relied on developing a new type of adversary method, called the inner-product adversary method.  Instead, we rely simply on the lower bound for unstructured search plus our quantitative no cloning theorem.

\subsection{Related Works}

\paragraph{Quantum Money.} Lutomirski~\cite{Lutomirski10} shows another weakness of Wiesner's scheme: a merchant, who is allowed to interact with the mint for verification, can use the verification oracle to break the scheme and forge new currency. Public key quantum money is necessarily secure against adversaries with a verification oracle, since the adversary can implement the verification oracle for itself.  Several alternative solutions to the limitations of Wiesner's scheme have been proposed~\cite{MosSte10,Gavinsky11}, though the ``ideal'' solution still remains public key quantum money.

\paragraph{Randomness Expansion.}  Colbeck~\cite{Colbeck09} proposed the idea of a classical experimenter, interacting with several potentially untrustworthy quantum devices, can expand a small random seed into a certifiably random longer seed.  Here, a crucial assumption is that the devices cannot communicate and must obey the laws of quantum mechanics; no other assumption about the devices is made.  The application of quantum lightning to verified randomness has a similar spirit, though the requirements are quite different.  Randomness expansion requires multiple non-communicating devices, but the experimenter can be classical and the devices can have unbounded computational power; in contrast quantum lightning involves only a single device, but the device must be computationally bounded, and the experimenter must perform quantum operations.  We note that a quantum experimenter can generate a random string for free; the purpose of verifiable entropy in this case is simply to prove to another individual that the coins you generated were indeed random.

\paragraph{Obfuscation and Multilinear Maps.}  There is a vast body of literature on strong notions of obfuscation, starting with the definitional work of Barak et al.~\cite{C:BGIRSVY01}.  Garg et al.~\cite{FOCS:GGHRSW13} propose the first obfuscator plausibly meeting the strong notion of iO, based on cryptographic multilinear maps~\cite{EC:GarGenHal13,C:CorLepTib13,TCC:GenGorHal15}.  Unfortunately, there have been numerous attacks on multilinear maps, which we do not fully elaborate on here.  Importantly, all current obfuscators are subject to very strong quantum attacks~\cite{EC:CDPR16,C:AlbBaiDuc16,CJL16,EC:CheGenHal17}, casting doubt on their quantum security.  However, there has been some success in transforming applications of obfuscation to be secure under assumptions on lattices~\cite{ITCS:BVWW16,EPRINT:WicZir17,EPRINT:GoyKopWat17}, which are widely believed to be quantum hard.   We therefore think it plausible that subspace-hiding obfuscation, which is all we need for this work, can be based on similar lattice problems.  Nonetheless, obfuscation is a very active area of research, and we believe that obfuscation secure against quantum attacks will likely be discovered in the near future.

\paragraph{Computational No-cloning.} We note that computational assumptions and no-cloning have been combined in other contexts, such as Unruh's revocable time-released encryption~\cite{EC:Unruh14}.  We note however, that these settings do not involve verification, the central theme of this work.
