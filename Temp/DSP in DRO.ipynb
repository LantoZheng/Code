{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cd58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import dsp\n",
    "import mosek\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from numpy.linalg import det, solve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm  # Import tqdm for progress display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f032dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CVXPY call this the logistic function\n",
    "def logistic(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63ab8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def DRO(X, y, K, max_iter=1000, tol=1e-6, eps=0.5, P_list=None, theta0_star=None, model=\"CN\"):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    X: list of numpy arrays, each element is a matrix corresponding to different domains.\n",
    "    y: list of numpy arrays, response variables for each domain (not used in this function).\n",
    "    K: int, number of classes.\n",
    "    max_iter: int, maximum number of iterations.\n",
    "    tol: float, tolerance for convergence.\n",
    "    eps: float, learning rate for gradient descent.\n",
    "    P_list: list of numpy arrays, probabilities for logistic regression.\n",
    "    theta0_star: numpy array, initial theta.\n",
    "    model: str, one of \"ID\" or \"CN\", determining the update rule for theta.\n",
    "    \n",
    "    Returns:\n",
    "    A dictionary with theta, gamma, and the number of iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 检查 model 参数是否在允许的选项中\n",
    "    if model not in [\"ID\", \"CN\"]:\n",
    "        raise ValueError(f\"Invalid model '{model}'. Choose either 'ID' or 'CN'.\")\n",
    "    \n",
    "    L = len(X)\n",
    "    p = X[0].shape[1]\n",
    "    theta0 = np.zeros((p, K))\n",
    "    theta = theta0\n",
    "    n_Q = sum([x.shape[0] for x in X])\n",
    "    X_mat = np.vstack(X)  # Combine all X matrices into one big matrix\n",
    "    \n",
    "    gamma0 = np.ones(L) / L\n",
    "\n",
    "    # Step 3: Main loop\n",
    "    mu_list = []\n",
    "    for l in range(L):\n",
    "        Xl = X[l]\n",
    "        Pl = P_list[l]\n",
    "        mu_list.append(Xl.T @ Pl / n_Q)\n",
    "    \n",
    "    mu_list_k = [np.column_stack([mu_list[l][:, k] for l in range(L)]) for k in range(K)]\n",
    "    \n",
    "    iter = 0\n",
    "    s = 1\n",
    "    gamma = gamma0\n",
    "    theta = theta0_star if theta0_star is not None else np.zeros((p, K))\n",
    "    \n",
    "    while s > tol and iter < max_iter:\n",
    "        iter += 1\n",
    "        \n",
    "        # Step 3.1: Compute Exp and q_mat\n",
    "        Exp = np.exp(X_mat @ theta - np.max(X_mat @ theta, axis=1, keepdims=True))\n",
    "        Sexp = np.sum(Exp, axis=1)\n",
    "        q_mat = Exp / Sexp[:, None]\n",
    "        \n",
    "        # Step 3.2: Compute v_mat\n",
    "        v_mat = X_mat.T @ q_mat / n_Q\n",
    "        \n",
    "        # Step 3.3: Compute Sig_list\n",
    "        Sig_list = []\n",
    "        for k in range(K):\n",
    "            Dkk = np.diag(q_mat[:, k] - q_mat[:, k]**2)\n",
    "            Sig_k = X_mat.T @ Dkk @ X_mat / n_Q\n",
    "            while det(Sig_k) <= 1e-2:\n",
    "                Sig_k += 1e-2 * np.eye(p)\n",
    "            Sig_list.append(Sig_k)\n",
    "        \n",
    "        # Step 3.4: Update gamma using CVXPY\n",
    "        A = np.zeros((L, L))\n",
    "        for k in range(K):\n",
    "            A += mu_list_k[k].T @ solve(Sig_list[k], mu_list_k[k])\n",
    "        \n",
    "        while det(A) <= 1e-2:\n",
    "            A += 1e-3 * np.eye(L)\n",
    "        \n",
    "        b = np.zeros(L)\n",
    "        for k in range(K):\n",
    "            b += mu_list_k[k].T @ solve(Sig_list[k], v_mat[:, k])\n",
    "        \n",
    "        gamma_var = cp.Variable(L)\n",
    "        objective = cp.Minimize(0.5 * cp.quad_form(gamma_var, A) - b @ gamma_var)\n",
    "        constraints = [cp.sum(gamma_var) == 1, gamma_var >= 0]\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        prob.solve()\n",
    "        gamma = gamma_var.value\n",
    "        \n",
    "        # Step 3.5: Update theta based on the selected model\n",
    "        theta_old = theta.copy()\n",
    "        for k in range(1, K):\n",
    "            grad = mu_list_k[k] @ gamma - v_mat[:, k]\n",
    "            if model == \"CN\":\n",
    "                theta[:, k] += eps * solve(Sig_list[k], grad)\n",
    "            elif model == \"ID\":\n",
    "                theta[:, k] += eps * grad\n",
    "        \n",
    "        # Step 3.6: Update s (convergence check)\n",
    "        s = np.sqrt(np.sum((theta_old - theta) ** 2))\n",
    "    \n",
    "    gamma = np.array(gamma).flatten()\n",
    "    return {'theta': theta, 'gamma': gamma, 'iter': iter}\n",
    "\n",
    "# 示例用法\n",
    "# 定义一些模拟数据\n",
    "#X = [np.random.randn(10, 3), np.random.randn(15, 3)]  # 两个域的特征矩阵\n",
    "#y = [np.random.randint(0, 2, 10), np.random.randint(0, 2, 15)]  # 响应变量 (在本函数中不使用)\n",
    "#P_list = [np.random.rand(10, 2), np.random.rand(15, 2)]  # 对数几率模型的概率\n",
    "#theta0_star = np.random.rand(3, 2)  # 初始 theta\n",
    "\n",
    "# 选择模型 \"CN\" 或 \"ID\"\n",
    "#result_id = DRO(X, y, 2, P_list=P_list, theta0_star=theta0_star, model=\"ID\")\n",
    "#print(result_id)\n",
    "#result_cn = DRO(X, y, 2, P_list=P_list, theta0_star=theta0_star, model=\"CN\")\n",
    "#print(result_cn)\n",
    "\n",
    "# 示例调用\n",
    "#X = [np.random.randn(100, 10) for _ in range(3)]\n",
    "#y = [np.random.randint(0, 2, 100) for _ in range(3)]\n",
    "#K = 3\n",
    "#P_list = [np.random.rand(100, K) for _ in range(3)]\n",
    "#theta0_star = np.random.rand(10, K)\n",
    "#result_id = DRO(X, y, K, P_list=P_list, theta0_star=theta0_star,model=\"ID\")\n",
    "#print(result_id)\n",
    "#result_cn = DRO(X, y, K, P_list=P_list, theta0_star=theta0_star,model=\"CN\")\n",
    "#print(result_cn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41028bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_predictions(X, theta):\n",
    "    if np.ndim(theta) == 1:\n",
    "        theta_column = np.reshape(theta,(len(theta),1))\n",
    "    else:\n",
    "        theta_column = theta\n",
    "    zeros_column = np.zeros((len(theta),1))\n",
    "    theta_cc = np.hstack([zeros_column,theta_column]) \n",
    "    # calculate logits (n * K)\n",
    "    logits = np.dot(X, theta_cc)\n",
    "    \n",
    "    # calculate softmax probability p_pred (n * K)\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  \n",
    "    p_pred = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    \n",
    "    # classification prediction y_pred (n, )\n",
    "    y_pred = np.argmax(p_pred, axis=1)\n",
    "    \n",
    "    return p_pred, y_pred\n",
    "\n",
    "# eg\n",
    "# X = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]) \n",
    "# theta = np.array([[0.5, -0.5], [1.0, -1.0]])  \n",
    "# theta = np.array([0.5, -0.5]) \n",
    "# p_pred, y_pred = calculate_predictions(X, theta)\n",
    "# print(\"p_pred:\\n\", p_pred)\n",
    "# print(\"y_pred:\\n\", y_pred) # from [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afbca9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred_prob):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss.\n",
    "\n",
    "    Parameters:\n",
    "    y_true: numpy array of shape (n_samples, 1), the true labels (0 or 1).\n",
    "    y_pred_prob: numpy array of shape (n_samples, 2), the predicted probabilities for each class (class 0 and class 1).\n",
    "\n",
    "    Returns:\n",
    "    Cross-entropy loss as a float.\n",
    "    \"\"\"\n",
    "    # Ensure that y_pred_prob values are within (0, 1) to avoid log overflow.\n",
    "    epsilon = 1e-15\n",
    "    y_pred_prob = np.clip(y_pred_prob, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Extract the probabilities corresponding to the positive class (class 1).\n",
    "    y_pred_prob_positive = y_pred_prob[:, 1]\n",
    "    \n",
    "    # Calculate the cross-entropy loss.\n",
    "    loss = -np.mean(y_true * np.log(y_pred_prob_positive) + (1 - y_true) * np.log(1 - y_pred_prob_positive))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def zero_one_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the 0-1 loss (error rate).\n",
    "\n",
    "    Parameters:\n",
    "    y_true: numpy array of shape (n_samples, 1), the true labels.\n",
    "    y_pred: numpy array of shape (n_samples, 1), the predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    0-1 loss (error rate) as a float.\n",
    "    \"\"\"\n",
    "    # Compute the error rate.\n",
    "    error_rate = np.mean(y_true != y_pred)\n",
    "    \n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292b0625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=  0\n",
      "(1500, 3)\n",
      "(450, 3)\n",
      "(1050, 3)\n",
      "0\n",
      "(1500, 3)\n",
      "(450, 3)\n",
      "(1050, 3)\n",
      "1\n",
      "(1500, 3)\n",
      "(450, 3)\n",
      "(1050, 3)\n",
      "2\n",
      "(1500, 3)\n",
      "(450, 3)\n",
      "(1050, 3)\n",
      "3\n",
      "(1500, 3)\n",
      "(450, 3)\n",
      "(1050, 3)\n",
      "4\n",
      "(1500, 3)\n",
      "(450, 3)\n",
      "(1050, 3)\n",
      "5\n",
      "(1500, 3)\n",
      "(450, 3)\n",
      "(1050, 3)\n",
      "6\n",
      "(1500, 3)\n",
      "(450, 3)\n",
      "(1050, 3)\n",
      "7\n",
      "(1500, 3)\n",
      "(450, 3)\n",
      "(1050, 3)\n",
      "8\n",
      "(1500, 3)\n",
      "(450, 3)\n",
      "(1050, 3)\n",
      "9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (450,) (4500,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 225\u001b[0m\n\u001b[0;32m    222\u001b[0m zero_one_loss_cn_dsp_train\u001b[38;5;241m.\u001b[39mappend(zero_one_loss(np\u001b[38;5;241m.\u001b[39mhstack(y_train_list), yFit_cn[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# cross-entropy loss\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m cross_entropy_loss_id_dsp_test\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpPred_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    226\u001b[0m cross_entropy_loss_cn_dsp_test\u001b[38;5;241m.\u001b[39mappend(cross_entropy_loss(np\u001b[38;5;241m.\u001b[39mhstack(y_test_list), np\u001b[38;5;241m.\u001b[39mvstack(pPred_cn)))\n\u001b[0;32m    227\u001b[0m cross_entropy_loss_id_dsp_train\u001b[38;5;241m.\u001b[39mappend(cross_entropy_loss(np\u001b[38;5;241m.\u001b[39mhstack(y_train_list), np\u001b[38;5;241m.\u001b[39mvstack(pFit_id)))\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mcross_entropy_loss\u001b[1;34m(y_true, y_pred_prob)\u001b[0m\n\u001b[0;32m     17\u001b[0m y_pred_prob_positive \u001b[38;5;241m=\u001b[39m y_pred_prob[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate the cross-entropy loss.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_prob_positive\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_true) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_pred_prob_positive))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (450,) (4500,) "
     ]
    }
   ],
   "source": [
    "\n",
    "Error_var_id = []\n",
    "Sd_var_id = []\n",
    "Error_var_cn = []\n",
    "Sd_var_cn = []\n",
    "\n",
    "zero_one_loss_id_dsp_test = []\n",
    "zero_one_loss_cn_dsp_test = []\n",
    "zero_one_loss_id_dsp_train = []\n",
    "zero_one_loss_cn_dsp_train = []\n",
    "\n",
    "cross_entropy_loss_id_dsp_test = []\n",
    "cross_entropy_loss_cn_dsp_test = []\n",
    "cross_entropy_loss_id_dsp_train = []\n",
    "cross_entropy_loss_cn_dsp_train = []\n",
    "\n",
    "\n",
    "size = 20 \n",
    "base = 500\n",
    "rep = 10\n",
    "p = 3\n",
    "\n",
    "beta_1 = np.array([0.9,0.2,0.2])\n",
    "beta_2 = np.array([0.2,0.9,0.2])\n",
    "beta_3 = np.array([0.2,0.2,0.9])\n",
    "\n",
    "for j in range(size):\n",
    "    print(\"j= \",j)\n",
    "    importance = [] ## \n",
    "    weighting = [] ## gamma\n",
    "    variable = [] ## theta\n",
    "    wei_id = []\n",
    "    wei_cn = []\n",
    "    var_id = []\n",
    "    var_cn = []\n",
    "    yPred_id = []\n",
    "    yPred_cn = []\n",
    "    yPred_dsp = []\n",
    "    pPred_id = []\n",
    "    pPred_cn = []\n",
    "    pPred_dsp = []\n",
    "    y_test_true = []\n",
    "    y_train_true = []\n",
    "    y_Pred_id_list = []\n",
    "    y_Pred_cn_list = []\n",
    "    y_Pred_dsp_list = []\n",
    "    y_True_list = []\n",
    "    p_Pred_id_list = []\n",
    "    p_Pred_cn_list = []\n",
    "    p_Pred_dsp_list = []\n",
    "    yFit_dsp = []\n",
    "    yFit_id = []\n",
    "    yFit_cn = []\n",
    "    pFit_dsp = []\n",
    "    pFit_id = []\n",
    "    pFit_cn = []\n",
    "    for i in range(rep):\n",
    "        n = base*(j+1)\n",
    "        X_1 = np.random.uniform(-10, 10, (n, 3))\n",
    "        X_2 = np.random.uniform(-10, 10, (n, 3))\n",
    "        X_3 = np.random.uniform(-10, 10, (n, 3))\n",
    "\n",
    "        logits_1 = X_1.dot(beta_1)\n",
    "        probs_1 = sigmoid(logits_1)\n",
    "        y_1 = np.random.binomial(1, probs_1)\n",
    "\n",
    "        logits_2 = X_2.dot(beta_2)\n",
    "        probs_2 = sigmoid(logits_2)\n",
    "        y_2 = np.random.binomial(1, probs_2)\n",
    "\n",
    "        logits_3 = X_3.dot(beta_3)\n",
    "        probs_3 = sigmoid(logits_3)\n",
    "        y_3 = np.random.binomial(1, probs_3)\n",
    "\n",
    "        X = [X_1,X_2,X_3]\n",
    "        X_mat = np.vstack(X)\n",
    "        print(np.shape(X_mat))\n",
    "        y = [y_1,y_2,y_3]\n",
    "\n",
    "        # data splitting\n",
    "        X_train_list = []\n",
    "        X_test_list = []\n",
    "        y_train_list = []\n",
    "        y_test_list = []\n",
    "\n",
    "        for X_i, y_i in zip(X, y):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_i, y_i, test_size=0.3, random_state=42)\n",
    "            X_train_list.append(X_train)\n",
    "            X_test_list.append(X_test)\n",
    "            y_train_list.append(y_train)\n",
    "            y_test_list.append(y_test)\n",
    "\n",
    "        y_test_true.append(np.vstack(y_test_list))\n",
    "        y_train_true.append(np.vstack(y_train_list))\n",
    "        X_test_mat = np.vstack(X_test_list)\n",
    "        print(np.shape(X_test_mat))\n",
    "        X_train_mat = np.vstack(X_train_list)\n",
    "        print(np.shape(X_train_mat))\n",
    "        \n",
    "        \n",
    "        model1=LogisticRegression()\n",
    "        model2=LogisticRegression()\n",
    "        model3=LogisticRegression()\n",
    "        model1.fit(X_train_list[0], y_train_list[0])\n",
    "        model2.fit(X_train_list[1], y_train_list[1])\n",
    "        model3.fit(X_train_list[2], y_train_list[2])\n",
    "        phat_1=model1.predict_proba(X_train_list[0])\n",
    "        phat_2=model2.predict_proba(X_train_list[1])\n",
    "        phat_3=model3.predict_proba(X_train_list[2])\n",
    "\n",
    "\n",
    "        P_list = [phat_1,phat_2,phat_3]\n",
    "        K=2\n",
    "        theta0_star = np.random.rand(p, K)\n",
    "        ## DRO sol\n",
    "        res_id = DRO(X_train_list, y_train_list, K, P_list=P_list, theta0_star=theta0_star, model=\"ID\")\n",
    "        res_cn = DRO(X_train_list, y_train_list, K, P_list=P_list, theta0_star=theta0_star, model=\"CN\")\n",
    "        var_id.append(res_id[\"theta\"][:, 1:])\n",
    "        var_cn.append(res_cn[\"theta\"][:, 1:])\n",
    "        wei_id.append(res_id[\"gamma\"])\n",
    "        wei_cn.append(res_cn[\"gamma\"])\n",
    "        p_pred,y_pred = calculate_predictions(X_test_mat, res_id[\"theta\"][:, 1:])\n",
    "        ppred_id = p_pred\n",
    "        pPred_id.append(ppred_id)\n",
    "        ypred_id = y_pred\n",
    "        yPred_id.append(ypred_id)\n",
    "        p_pred,y_pred = calculate_predictions(X_train_mat, res_id[\"theta\"][:, 1:])\n",
    "        pfit_id = p_pred\n",
    "        pFit_id.append(pfit_id)\n",
    "        yfit_id = y_pred\n",
    "        yFit_id.append(yfit_id)\n",
    "        p_pred,y_pred = calculate_predictions(X_test_mat, res_cn[\"theta\"][:, 1:])\n",
    "        ypred_cn = y_pred\n",
    "        yPred_cn.append(ypred_cn)\n",
    "        ppred_cn = p_pred\n",
    "        pPred_cn.append(ppred_cn)\n",
    "        p_pred,y_pred = calculate_predictions(X_train_mat, res_cn[\"theta\"][:, 1:])\n",
    "        yfit_cn = y_pred\n",
    "        yFit_cn.append(yfit_cn)\n",
    "        pfit_cn = p_pred\n",
    "        pFit_cn.append(pfit_cn)\n",
    "        \n",
    "        \n",
    "        ## DSP sol\n",
    "        q = cp.Variable(3, nonneg=True) # gamma\n",
    "        b = cp.Variable(3) # theta\n",
    "        b_column = cp.reshape(b, (3, 1))\n",
    "        zero_column = np.zeros((3, 1))\n",
    "        B = cp.hstack([zero_column,b_column])\n",
    "\n",
    "        logodds_1 = X_train_list[0] @ B\n",
    "        logodds_2 = X_train_list[1] @ B\n",
    "        logodds_3 = X_train_list[2] @ B\n",
    "        logodds_vs = cp.vstack([X_train_list[0],X_train_list[1],X_train_list[2]])@ B \n",
    "\n",
    "\n",
    "        imp_1 = cp.sum(cp.multiply(phat_1, logodds_1))\n",
    "\n",
    "        imp_2 = cp.sum(cp.multiply(phat_2, logodds_2))\n",
    "\n",
    "        imp_3 = cp.sum(cp.multiply(phat_3, logodds_3))\n",
    "\n",
    "        f_1 = dsp.saddle_inner(q,cp.hstack([imp_1,imp_2,imp_3])/ (0.7*n))\n",
    "\n",
    "        f_2 = cp.sum(cp.log_sum_exp(logodds_vs,axis=1))/(n*3*0.7)\n",
    "\n",
    "        f=f_1-f_2\n",
    "\n",
    "        obj = dsp.MinimizeMaximize(f)\n",
    "        constraints = [cp.sum(q) == 1]\n",
    "        prob = dsp.SaddlePointProblem(obj,constraints,[q],[b])\n",
    "\n",
    "        try:\n",
    "            prob.solve(solver=\"MOSEK\")\n",
    "            importance.append(prob.value)\n",
    "            weighting.append(q.value)\n",
    "            variable.append(b.value)\n",
    "            p_pred,y_pred = calculate_predictions(X_test_mat, b.value)\n",
    "            ypred_dsp = y_pred\n",
    "            yPred_dsp.append(ypred_dsp)\n",
    "            ppred_dsp = p_pred\n",
    "            pPred_dsp.append(ppred_dsp)\n",
    "            p_pred,y_pred = calculate_predictions(X_train_mat, b.value)\n",
    "            yfit_dsp = y_pred\n",
    "            yFit_dsp.append(yfit_dsp)\n",
    "            pfit_dsp = p_pred\n",
    "            pFit_dsp.append(pfit_dsp)\n",
    "            print(i)\n",
    "\n",
    "            #site_1 = y_1*(X_1 @ b.value) - logistic(X_1 @ b.value) - y_1*np.log(p1_bar) - (1-y_1)*np.log(1-p1_bar)\n",
    "            #site_2 = y_2*(X_2 @ b.value) - logistic(X_2 @ b.value) - y_2*np.log(p2_bar) - (1-y_2)*np.log(1-p2_bar)\n",
    "            #site_3 = y_3*(X_3 @ b.value) - logistic(X_3 @ b.value) - y_3*np.log(p3_bar) - (1-y_3)*np.log(1-p3_bar)\n",
    "\n",
    "            #site_importance = np.vstack([site_1,site_2,site_3])\n",
    "            #site_importance = site_importance.T\n",
    "            #cov = np.cov(site_importance, rowvar = False)\n",
    "\n",
    "            #var = np.dot(q.value, np.dot(cov, q.value.T))\n",
    "            #std_clt = np.sqrt(var/1000)\n",
    "            #stds.append(std_clt)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on iteration {i}: {e}\")\n",
    "            continue\n",
    "     \n",
    "\n",
    "    \n",
    "    \n",
    "    # parameter error\n",
    "    error_var_id = np.mean((np.hstack(var_id).T-np.vstack(variable))**2)\n",
    "    Error_var_id.append(error_var_id)\n",
    "    sd_var_id = np.sqrt(np.var(np.hstack(var_id)))\n",
    "    Sd_var_id.append(sd_var_id)\n",
    "    error_var_cn = np.mean((np.hstack(var_cn).T-np.vstack(variable))**2)\n",
    "    Error_var_cn.append(error_var_cn)\n",
    "    sd_var_cn = np.sqrt(np.var(np.hstack(var_cn)))\n",
    "    Sd_var_cn.append(sd_var_cn)\n",
    "    \n",
    "    # 0-1 loss\n",
    "    zero_one_loss_id_dsp_test.append(zero_one_loss(np.hstack(y_test_list), yPred_id[-1]))\n",
    "    zero_one_loss_cn_dsp_test.append(zero_one_loss(np.hstack(y_test_list), yPred_cn[-1]))\n",
    "    zero_one_loss_id_dsp_train.append(zero_one_loss(np.hstack(y_train_list), yFit_id[-1]))\n",
    "    zero_one_loss_cn_dsp_train.append(zero_one_loss(np.hstack(y_train_list), yFit_cn[-1]))\n",
    "\n",
    "    # cross-entropy loss\n",
    "    cross_entropy_loss_id_dsp_test.append(cross_entropy_loss(np.hstack(y_test_list), np.vstack(pPred_id)))\n",
    "    cross_entropy_loss_cn_dsp_test.append(cross_entropy_loss(np.hstack(y_test_list), np.vstack(pPred_cn)))\n",
    "    cross_entropy_loss_id_dsp_train.append(cross_entropy_loss(np.hstack(y_train_list), np.vstack(pFit_id)))\n",
    "    cross_entropy_loss_cn_dsp_train.append(cross_entropy_loss(np.hstack(y_train_list), np.vstack(pFit_cn)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b5969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run experiments for a given sample size\n",
    "def run_experiment(j):\n",
    "    print(f\"Running experiment for sample size index {j}\")\n",
    "    \n",
    "    # Result containers for each repetition\n",
    "    var_id, var_cn, variable = [], [], []\n",
    "    yPred_id, yPred_cn, yPred_dsp = [], [], []\n",
    "    pPred_id, pPred_cn, pPred_dsp = [], [], []\n",
    "    yFit_id, yFit_cn, yFit_dsp = [], [], []\n",
    "    pFit_id, pFit_cn, pFit_dsp = [], [], []\n",
    "    y_test_true, y_train_true = [], []\n",
    "\n",
    "    for i in range(rep):\n",
    "        n = base * (j + 1)\n",
    "        X_1 = np.random.uniform(-10, 10, (n, 3))\n",
    "        X_2 = np.random.uniform(-10, 10, (n, 3))\n",
    "        X_3 = np.random.uniform(-10, 10, (n, 3))\n",
    "\n",
    "        logits_1 = X_1.dot(beta_1)\n",
    "        probs_1 = sigmoid(logits_1)\n",
    "        y_1 = np.random.binomial(1, probs_1)\n",
    "\n",
    "        logits_2 = X_2.dot(beta_2)\n",
    "        probs_2 = sigmoid(logits_2)\n",
    "        y_2 = np.random.binomial(1, probs_2)\n",
    "\n",
    "        logits_3 = X_3.dot(beta_3)\n",
    "        probs_3 = sigmoid(logits_3)\n",
    "        y_3 = np.random.binomial(1, probs_3)\n",
    "\n",
    "        X = [X_1, X_2, X_3]\n",
    "        y = [y_1, y_2, y_3]\n",
    "\n",
    "        # Data splitting\n",
    "        X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n",
    "\n",
    "        for X_i, y_i in zip(X, y):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_i, y_i, test_size=0.3, random_state=42)\n",
    "            X_train_list.append(X_train)\n",
    "            X_test_list.append(X_test)\n",
    "            y_train_list.append(y_train)\n",
    "            y_test_list.append(y_test)\n",
    "\n",
    "        y_test_true.append(np.vstack(y_test_list))\n",
    "        y_train_true.append(np.vstack(y_train_list))\n",
    "        X_test_mat = np.vstack(X_test_list)\n",
    "        X_train_mat = np.vstack(X_train_list)\n",
    "\n",
    "        # Train logistic regression models\n",
    "        model1 = LogisticRegression()\n",
    "        model2 = LogisticRegression()\n",
    "        model3 = LogisticRegression()\n",
    "        model1.fit(X_train_list[0], y_train_list[0])\n",
    "        model2.fit(X_train_list[1], y_train_list[1])\n",
    "        model3.fit(X_train_list[2], y_train_list[2])\n",
    "        phat_1 = model1.predict_proba(X_train_list[0])\n",
    "        phat_2 = model2.predict_proba(X_train_list[1])\n",
    "        phat_3 = model3.predict_proba(X_train_list[2])\n",
    "\n",
    "        P_list = [phat_1, phat_2, phat_3]\n",
    "        K = 2\n",
    "        theta0_star = np.random.rand(p, K)\n",
    "\n",
    "        # DRO solutions\n",
    "        res_id = DRO(X_train_list, y_train_list, K, P_list=P_list, theta0_star=theta0_star, model=\"ID\")\n",
    "        res_cn = DRO(X_train_list, y_train_list, K, P_list=P_list, theta0_star=theta0_star, model=\"CN\")\n",
    "        var_id.append(res_id[\"theta\"][:, 1:])\n",
    "        var_cn.append(res_cn[\"theta\"][:, 1:])\n",
    "\n",
    "        # ID model predictions\n",
    "        p_pred, y_pred = calculate_predictions(X_test_mat, res_id[\"theta\"][:, 1:])\n",
    "        pPred_id.append(p_pred)\n",
    "        yPred_id.append(y_pred)\n",
    "        p_pred, y_pred = calculate_predictions(X_train_mat, res_id[\"theta\"][:, 1:])\n",
    "        pFit_id.append(p_pred)\n",
    "        yFit_id.append(y_pred)\n",
    "\n",
    "        # CN model predictions\n",
    "        p_pred, y_pred = calculate_predictions(X_test_mat, res_cn[\"theta\"][:, 1:])\n",
    "        pPred_cn.append(p_pred)\n",
    "        yPred_cn.append(y_pred)\n",
    "        p_pred, y_pred = calculate_predictions(X_train_mat, res_cn[\"theta\"][:, 1:])\n",
    "        pFit_cn.append(p_pred)\n",
    "        yFit_cn.append(y_pred)\n",
    "\n",
    "        # DSP solution\n",
    "        q = cp.Variable(3, nonneg=True)  # gamma\n",
    "        b = cp.Variable(3)  # theta\n",
    "        b_column = cp.reshape(b, (3, 1))\n",
    "        zero_column = np.zeros((3, 1))\n",
    "        B = cp.hstack([zero_column, b_column])\n",
    "\n",
    "        logodds_1 = X_train_list[0] @ B\n",
    "        logodds_2 = X_train_list[1] @ B\n",
    "        logodds_3 = X_train_list[2] @ B\n",
    "        logodds_vs = cp.vstack([X_train_list[0], X_train_list[1], X_train_list[2]]) @ B \n",
    "\n",
    "        imp_1 = cp.sum(cp.multiply(phat_1, logodds_1))\n",
    "        imp_2 = cp.sum(cp.multiply(phat_2, logodds_2))\n",
    "        imp_3 = cp.sum(cp.multiply(phat_3, logodds_3))\n",
    "\n",
    "        f_1 = cp.sum(cp.hstack([imp_1, imp_2, imp_3])) / (0.7 * n)\n",
    "        f_2 = cp.sum(cp.log_sum_exp(logodds_vs, axis=1)) / (n * 3 * 0.7)\n",
    "        f = f_1 - f_2\n",
    "\n",
    "        obj = cp.Minimize(f)\n",
    "        constraints = [cp.sum(q) == 1]\n",
    "        prob = cp.Problem(obj, constraints)\n",
    "\n",
    "        try:\n",
    "            prob.solve(solver=\"MOSEK\")\n",
    "            variable.append(b.value)\n",
    "            p_pred, y_pred = calculate_predictions(X_test_mat, b.value)\n",
    "            pPred_dsp.append(p_pred)\n",
    "            yPred_dsp.append(y_pred)\n",
    "            p_pred, y_pred = calculate_predictions(X_train_mat, b.value)\n",
    "            pFit_dsp.append(p_pred)\n",
    "            yFit_dsp.append(y_pred)\n",
    "        except Exception as e:\n",
    "            print(f\"Error on DSP solve iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Calculate errors and losses\n",
    "    error_var_id = np.mean((np.hstack(var_id).T - np.vstack(variable))**2)\n",
    "    error_var_cn = np.mean((np.hstack(var_cn).T - np.vstack(variable))**2)\n",
    "    sd_var_id = np.sqrt(np.var(np.hstack(var_id)))\n",
    "    sd_var_cn = np.sqrt(np.var(np.hstack(var_cn)))\n",
    "\n",
    "    zero_one_loss_id_dsp_test = zero_one_loss(np.hstack(y_test_true), yPred_id[-1])\n",
    "    zero_one_loss_cn_dsp_test = zero_one_loss(np.hstack(y_test_true), yPred_cn[-1])\n",
    "    zero_one_loss_id_dsp_train = zero_one_loss(np.hstack(y_train_true), yFit_id[-1])\n",
    "    zero_one_loss_cn_dsp_train = zero_one_loss(np.hstack(y_train_true), yFit_cn[-1])\n",
    "\n",
    "    cross_entropy_loss_id_dsp_test = cross_entropy_loss(np.hstack(y_test_true), np.vstack(pPred_id))\n",
    "    cross_entropy_loss_cn_dsp_test = cross_entropy_loss(np.hstack(y_test_true), np.vstack(pPred_cn))\n",
    "    cross_entropy_loss_id_dsp_train = cross_entropy_loss(np.hstack(y_train_true), np.vstack(pFit_id))\n",
    "    cross_entropy_loss_cn_dsp_train = cross_entropy_loss(np.hstack(y_train_true), np.vstack(pFit_cn))\n",
    "\n",
    "    return {\n",
    "            \"Error_var_id\": error_var_id,\n",
    "            \"Sd_var_id\": sd_var_id,\n",
    "            \"Error_var_cn\": error_var_cn,\n",
    "            \"Sd_var_cn\": sd_var_cn,\n",
    "            \"Zero_one_loss_id_dsp_test\": zero_one_loss_id_dsp_test,\n",
    "            \"Zero_one_loss_cn_dsp_test\": zero_one_loss_cn_dsp_test,\n",
    "            \"Zero_one_loss_id_dsp_train\": zero_one_loss_id_dsp_train,\n",
    "            \"Zero_one_loss_cn_dsp_train\": zero_one_loss_cn_dsp_train,\n",
    "            \"Cross_entropy_loss_id_dsp_test\": cross_entropy_loss_id_dsp_test,\n",
    "            \"Cross_entropy_loss_cn_dsp_test\": cross_entropy_loss_cn_dsp_test,\n",
    "            \"Cross_entropy_loss_id_dsp_train\": cross_entropy_loss_id_dsp_train,\n",
    "            \"Cross_entropy_loss_cn_dsp_train\": cross_entropy_loss_cn_dsp_train,\n",
    "        }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c40aeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    size = 20\n",
    "    base = 500\n",
    "    rep = 10\n",
    "    p = 3\n",
    "    beta_1 = np.array([0.9, 0.2, 0.2])\n",
    "    beta_2 = np.array([0.2, 0.9, 0.2])\n",
    "    beta_3 = np.array([0.2, 0.2, 0.9])\n",
    "\n",
    "    # List to store results\n",
    "    results = []\n",
    "    print(\"set up\")\n",
    "    # Setup tqdm progress bar\n",
    "    with tqdm(total=size * rep) as pbar:\n",
    "        def update(*a):\n",
    "            pbar.update()\n",
    "\n",
    "        # Function to run experiments in parallel\n",
    "        def run_experiments_parallel():\n",
    "            with Pool(processes=None) as pool:\n",
    "                results = list(tqdm(pool.imap_unordered(run_experiment, range(size)), total=size))\n",
    "            return results\n",
    "\n",
    "        # Run experiments\n",
    "        print(\"start\")\n",
    "        results = run_experiments_parallel()\n",
    "\n",
    "    # Print or use results as needed\n",
    "    print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e97e8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sd of id:  0.006925646434470042\n",
      "sd of cn:  0.006925646434470042\n",
      "L-2 error of id:  0.00012396863395844335\n",
      "L-2 error of cn:  0.00012396863395844335\n"
     ]
    }
   ],
   "source": [
    "error_var_id = np.mean((np.hstack(var_id).T-np.vstack(variable))**2)/(0.7*n)/3\n",
    "sd_var_id = np.sqrt(np.var(np.hstack(var_id))/(0.7*n)/3)\n",
    "print(\"sd of id: \",sd_var_id)\n",
    "error_var_cn = np.mean((np.hstack(var_cn).T-np.vstack(variable))**2)/(0.7*n)/3\n",
    "sd_var_cn = np.sqrt(np.var(np.hstack(var_cn))/(0.7*n)/3)\n",
    "print(\"sd of cn: \",sd_var_cn)\n",
    "print(\"L-2 error of id: \",error_var_id)\n",
    "print(\"L-2 error of cn: \",error_var_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "077367b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1500 and the array at index 20 has size 3000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [71]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_Pred_id \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43myPred_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (300*3*10)*1\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#y_Pred_id = np.reshape(y_Pred_id.T,(30000,1))\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y_Pred_cn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(yPred_cn) \u001b[38;5;66;03m# (300*3*10)*1\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\numpy\\core\\shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    281\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1500 and the array at index 20 has size 3000"
     ]
    }
   ],
   "source": [
    "y_Pred_id = np.vstack(yPred_id) # (300*3*10)*1\n",
    "#y_Pred_id = np.reshape(y_Pred_id.T,(30000,1))\n",
    "y_Pred_cn = np.vstack(yPred_cn) # (300*3*10)*1\n",
    "#y_Pred_cn = np.reshape(y_Pred_cn.T,(30000,1))\n",
    "y_Pred_dsp = np.vstack(yPred_dsp) # (300*3*10)*1 \n",
    "#y_Pred_dsp = np.reshape(y_Pred_dsp.T,(30000,1))\n",
    "y_True = np.vstack(y_test_true) # (300*3*10)*1 对\n",
    "\n",
    "p_Pred_id = np.vstack(pPred_id) # (300*3*10)*2 ？ 30000*2\n",
    "p_Pred_cn = np.vstack(pPred_cn) # (300*3*10)*2\n",
    "p_Pred_dsp = np.vstack(pPred_dsp) # (300*3*10)*2\n",
    "\n",
    "\n",
    "# Cross-Entropy Loss\n",
    "# cel_id = cross_entropy_loss(y_True, p_Pred_id)\n",
    "# cel_cn = cross_entropy_loss(y_True, p_Pred_cn)\n",
    "# cel_dsp = cross_entropy_loss(y_True, p_Pred_dsp)\n",
    "#cel_id_dsp = cross_entropy_loss(y_Pred_dsp, p_Pred_id)\n",
    "#cel_cn_dsp = cross_entropy_loss(y_Pred_dsp, p_Pred_cn)\n",
    "#print(\"Cross-Entropy Loss, id-dsp: \", cel_id_dsp)\n",
    "#print(\"Cross-Entropy Loss, cn-dsp: \", cel_cn_dsp)\n",
    "\n",
    "\n",
    "# 0-1 loss\n",
    "bl_id_dsp = zero_one_loss(y_Pred_dsp, y_Pred_id)\n",
    "bl_cn_dsp = zero_one_loss(y_Pred_dsp, y_Pred_cn)\n",
    "print(\"0-1 loss, id-dsp: \", bl_id_dsp)\n",
    "print(\"0-1 loss, cn-dsp: \", bl_cn_dsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "edb835c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_Pred_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b66e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # new list\n",
    "    y_Pred_id = np.vstack(yPred_id) # (300*3*10)*1\n",
    "    #y_Pred_id = np.reshape(y_Pred_id.T,(30000,1))\n",
    "    y_Pred_cn = np.vstack(yPred_cn) # (300*3*10)*1\n",
    "    #y_Pred_cn = np.reshape(y_Pred_cn.T,(30000,1))\n",
    "    y_Pred_dsp = np.vstack(yPred_dsp) # (300*3*10)*1 \n",
    "    #y_Pred_dsp = np.reshape(y_Pred_dsp.T,(30000,1))\n",
    "    y_True = np.vstack(y_test_true) # (300*3*10)*1 对\n",
    "\n",
    "    p_Pred_id = np.vstack(pPred_id) # (300*3*10)*2 ？ 30000*2\n",
    "    p_Pred_cn = np.vstack(pPred_cn) # (300*3*10)*2\n",
    "    p_Pred_dsp = np.vstack(pPred_dsp) # (300*3*10)*2\n",
    "    y_Pred_id_list.append(y_Pred_id )\n",
    "    y_Pred_cn_list.append(y_Pred_cn)\n",
    "    y_Pred_dsp_list.append(y_Pred_dsp)\n",
    "    y_True_list.append(y_True)\n",
    "    p_Pred_id_list.append(p_Pred_id)\n",
    "    p_Pred_cn_list.append(p_Pred_cn)\n",
    "    p_Pred_dsp_list.append(p_Pred_dsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6d2e371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 300)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_True = np.vstack(y_test_true) # (300*3*10)*1\n",
    "np.shape(y_True)\n",
    "#p_Pred_id = np.vstack(pPred_id) # (300*3*10)*2\n",
    "#np.shape(p_Pred_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705c4c7",
   "metadata": {},
   "source": [
    "## Purely Homogenerous Regime\n",
    "L=3,p=4,K=2\n",
    "证明id足够，其他等价，step 2只有在初值离谱的情况下才有用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9eb5228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j= 0\n",
      "0\n",
      "j= 1\n",
      "0\n",
      "j= 2\n",
      "0\n",
      "j= 3\n",
      "0\n",
      "j= 4\n",
      "0\n",
      "j= 5\n",
      "0\n",
      "j= 6\n",
      "0\n",
      "j= 7\n",
      "0\n",
      "j= 8\n",
      "0\n",
      "j= 9\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "importance = [] ## \n",
    "weighting = [] ## gamma\n",
    "variable = [] ## theta\n",
    "wei_id = []\n",
    "wei_cn = []\n",
    "var_id = []\n",
    "var_cn = []\n",
    "stds = []\n",
    "\n",
    "size = 10 \n",
    "base = 10\n",
    "rep = 1\n",
    "p = 4\n",
    "\n",
    "beta_1 = np.array([0.5,-0.5,1,-1])\n",
    "beta_3 = beta_2 = beta_1\n",
    "\n",
    "for j in range(size):\n",
    "    print(\"j=\",j)\n",
    "    for i in range(rep):\n",
    "        n = base*(j+1)\n",
    "        X_3=X_2=X_1 = np.random.multivariate_normal(np.zeros(p), np.eye(p),n)\n",
    "\n",
    "        logits_3=logits_2=logits_1 = X_1.dot(beta_1)\n",
    "        probs_3=probs_2=probs_1 = sigmoid(logits_1)\n",
    "        y_3=y_2=y_1 = np.random.binomial(1, probs_1)\n",
    "\n",
    "\n",
    "        model1=LogisticRegression()\n",
    "        model2=LogisticRegression()\n",
    "        model3=LogisticRegression()\n",
    "        model1.fit(X_1, y_1)\n",
    "        model2.fit(X_2, y_2)\n",
    "        model3.fit(X_3, y_3)\n",
    "        phat_1=model1.predict_proba(X_1)\n",
    "        phat_2=model2.predict_proba(X_2)\n",
    "        phat_3=model3.predict_proba(X_3)\n",
    "\n",
    "        X = [X_1,X_2,X_3]\n",
    "        y = [y_1,y_2,y_3]\n",
    "        P_list = [phat_1,phat_2,phat_3]\n",
    "        K=2\n",
    "        theta0_star = np.random.rand(p, K)\n",
    "        ## DRO_id sol\n",
    "        res_id = DRO(X, y, K, P_list=P_list, theta0_star=theta0_star, model=\"ID\")\n",
    "        res_cn = DRO(X, y, K, P_list=P_list, theta0_star=theta0_star, model=\"CN\")\n",
    "        var_id.append(res_id[\"theta\"][:, 1:])\n",
    "        var_cn.append(res_cn[\"theta\"][:, 1:])\n",
    "        wei_id.append(res_id[\"gamma\"])\n",
    "        wei_cn.append(res_cn[\"gamma\"])\n",
    "\n",
    "        ## DSP sol\n",
    "        q = cp.Variable(3, nonneg=True) # gamma\n",
    "        b = cp.Variable(p) # theta\n",
    "        b_column = cp.reshape(b, (p, 1))\n",
    "        zero_column = np.zeros((p, 1))\n",
    "        B = cp.hstack([zero_column,b_column])\n",
    "\n",
    "        logodds_1 = X_1 @ B\n",
    "        logodds_2 = X_2 @ B\n",
    "        logodds_3 = X_3 @ B\n",
    "        logodds_vs = cp.vstack([X_1,X_2,X_3])@ B \n",
    "\n",
    "\n",
    "        imp_1 = cp.sum(cp.multiply(phat_1, logodds_1))\n",
    "\n",
    "        imp_2 = cp.sum(cp.multiply(phat_2, logodds_2))\n",
    "\n",
    "        imp_3 = cp.sum(cp.multiply(phat_3, logodds_3))\n",
    "\n",
    "        f_1 = dsp.saddle_inner(q,cp.hstack([imp_1,imp_2,imp_3])/ n)\n",
    "\n",
    "        f_2 = cp.sum(cp.log_sum_exp(logodds_vs,axis=1))/(n*3)\n",
    "\n",
    "        f=f_1-f_2\n",
    "\n",
    "        obj = dsp.MinimizeMaximize(f)\n",
    "        constraints = [cp.sum(q) == 1]\n",
    "        prob = dsp.SaddlePointProblem(obj,constraints,[q],[b])\n",
    "\n",
    "        try:\n",
    "            prob.solve(solver=\"MOSEK\")\n",
    "            importance.append(prob.value)\n",
    "            weighting.append(q.value)\n",
    "            variable.append(b.value)\n",
    "            print(i)\n",
    "\n",
    "            #site_1 = y_1*(X_1 @ b.value) - logistic(X_1 @ b.value) - y_1*np.log(p1_bar) - (1-y_1)*np.log(1-p1_bar)\n",
    "            #site_2 = y_2*(X_2 @ b.value) - logistic(X_2 @ b.value) - y_2*np.log(p2_bar) - (1-y_2)*np.log(1-p2_bar)\n",
    "            #site_3 = y_3*(X_3 @ b.value) - logistic(X_3 @ b.value) - y_3*np.log(p3_bar) - (1-y_3)*np.log(1-p3_bar)\n",
    "\n",
    "            #site_importance = np.vstack([site_1,site_2,site_3])\n",
    "            #site_importance = site_importance.T\n",
    "            #cov = np.cov(site_importance, rowvar = False)\n",
    "\n",
    "            #var = np.dot(q.value, np.dot(cov, q.value.T))\n",
    "            #std_clt = np.sqrt(var/1000)\n",
    "            #stds.append(std_clt)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on iteration {i}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b767d7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for j in range(size):\n",
    "    variable_j = variable[rep*(j-1):(rep*j),:]\n",
    "    var_id_j = var_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16b8dd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Expression(AFFINE, UNKNOWN, ())"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a8979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
