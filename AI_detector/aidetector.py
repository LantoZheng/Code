#!/usr/bin/env python3
"""
LaTeX AI Content Detector
é€šè¿‡è®¡ç®—è¯å…ƒçš„å›°æƒ‘åº¦ï¼ˆperplexityï¼‰æ¥æ£€æµ‹ LaTeX æ–‡æ¡£ä¸­å¯èƒ½ç”± AI ç”Ÿæˆçš„å†…å®¹
ä½¿ç”¨é¢œè‰²é«˜äº®æ˜¾ç¤ºï¼šè“è‰²ï¼ˆä½æ¦‚ç‡ï¼‰-> ç»¿è‰² -> é»„è‰² -> çº¢è‰²ï¼ˆé«˜æ¦‚ç‡ï¼‰
"""

import os
import re
import argparse
import numpy as np
from typing import List, Tuple, Optional
from openai import OpenAI


class LaTeXAIDetector:
    """LaTeX æ–‡æ¡£ AI å†…å®¹æ£€æµ‹å™¨"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-3.5-turbo", 
                 use_lmstudio: bool = False, lmstudio_url: str = "http://localhost:1234/v1"):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨
        
        Args:
            api_key: OpenAI API å¯†é’¥ï¼ˆuse_lmstudio=True æ—¶å¯é€‰ï¼‰
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            use_lmstudio: æ˜¯å¦ä½¿ç”¨ LMStudio æœ¬åœ°æ¨¡å‹
            lmstudio_url: LMStudio æœåŠ¡å™¨åœ°å€
        """
        self.use_lmstudio = use_lmstudio
        self.model = model
        
        if use_lmstudio:
            # ä½¿ç”¨ LMStudio æœ¬åœ°æœåŠ¡
            self.client = OpenAI(
                api_key="lm-studio",  # LMStudio ä¸éœ€è¦çœŸå®å¯†é’¥
                base_url=lmstudio_url
            )
            print(f"ğŸ  ä½¿ç”¨ LMStudio æœ¬åœ°æ¨¡å‹: {lmstudio_url}")
        else:
            # ä½¿ç”¨ OpenAI API
            self.api_key = api_key or os.getenv("OPENAI_API_KEY")
            if not self.api_key:
                raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡æˆ–ä¼ å…¥ api_key å‚æ•°ï¼Œæˆ–ä½¿ç”¨ --lmstudio é€‰é¡¹")
            
            self.client = OpenAI(api_key=self.api_key)
            print(f"â˜ï¸  ä½¿ç”¨ OpenAI API")
        
    def extract_text_from_latex(self, latex_content: str) -> List[str]:
        """
        ä» LaTeX æ–‡æ¡£ä¸­æå–æ–‡æœ¬å†…å®¹
        
        Args:
            latex_content: LaTeX æºä»£ç 
            
        Returns:
            List of tokens (words)
        """
        # ç§»é™¤æ³¨é‡Š
        lines = latex_content.split('\n')
        processed_lines = []
        
        for line in lines:
            # ä¿ç•™ % å‰çš„å†…å®¹ï¼ˆé™¤é % è¢«è½¬ä¹‰ï¼‰
            comment_pos = line.find('%')
            if comment_pos > 0 and line[comment_pos - 1] != '\\':
                line = line[:comment_pos]
            processed_lines.append(line)
        
        content = '\n'.join(processed_lines)
        
        # ç§»é™¤å¸¸è§çš„ LaTeX å‘½ä»¤ä½†ä¿ç•™æ–‡æœ¬
        # ç§»é™¤ \begin{} å’Œ \end{} ä½†ä¿ç•™å†…å®¹
        content = re.sub(r'\\begin\{[^}]+\}', '', content)
        content = re.sub(r'\\end\{[^}]+\}', '', content)
        
        # ç§»é™¤å¸¸è§çš„æ ¼å¼å‘½ä»¤ä½†ä¿ç•™å‚æ•°
        content = re.sub(r'\\(textbf|textit|emph|underline)\{([^}]+)\}', r'\2', content)
        content = re.sub(r'\\(section|subsection|subsubsection|chapter|paragraph)\{([^}]+)\}', r'\2', content)
        
        # ç§»é™¤å…¶ä»–å•è¡Œå‘½ä»¤
        content = re.sub(r'\\[a-zA-Z]+\s*', ' ', content)
        
        # ç§»é™¤æ•°å­¦æ¨¡å¼
        content = re.sub(r'\$[^$]+\$', ' [MATH] ', content)
        content = re.sub(r'\\\[[^\]]+\\\]', ' [MATH] ', content)
        
        # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²ï¼‰
        words = re.findall(r'\b\w+\b|[.,;:!?]', content)
        
        return words
    
    def calculate_token_perplexity(self, context: str, token: str) -> float:
        """
        è®¡ç®—ç»™å®šä¸Šä¸‹æ–‡ä¸­æŸä¸ªè¯å…ƒå‡ºç°çš„å›°æƒ‘åº¦
        
        Args:
            context: å‰æ–‡ä¸Šä¸‹æ–‡
            token: å¾…æ£€æµ‹çš„è¯å…ƒ
            
        Returns:
            å›°æƒ‘åº¦å€¼ï¼ˆè¶Šä½è¡¨ç¤ºè¶Šå¯èƒ½ç”± AI ç”Ÿæˆï¼‰
        """
        try:
            # æ„å»ºæç¤ºï¼Œè®©æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
            prompt = f"è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªæœ€å¯èƒ½å‡ºç°çš„è¯ã€‚åªè¿”å›ä¸€ä¸ªè¯ï¼Œä¸è¦è§£é‡Šã€‚\n\næ–‡æœ¬ï¼š{context}\n\nä¸‹ä¸€ä¸ªè¯ï¼š"
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬é¢„æµ‹åŠ©æ‰‹ï¼Œåªè¿”å›æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=10,
                temperature=0.1,
                logprobs=True,
                top_logprobs=20
            )
            
            # è·å– logprobs
            if response.choices[0].logprobs and response.choices[0].logprobs.content:
                top_logprobs = response.choices[0].logprobs.content[0].top_logprobs
                
                # æŸ¥æ‰¾ç›®æ ‡ token çš„ logprob
                token_lower = token.lower()
                for logprob_item in top_logprobs:
                    if logprob_item.token.lower().strip() == token_lower:
                        # è½¬æ¢ä¸ºæ¦‚ç‡
                        prob = np.exp(logprob_item.logprob)
                        # å›°æƒ‘åº¦ = 1 / æ¦‚ç‡
                        perplexity = 1.0 / prob if prob > 0 else float('inf')
                        return perplexity
                
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›ä¸€ä¸ªè¾ƒé«˜çš„å›°æƒ‘åº¦
                return 100.0
            
            return 50.0  # é»˜è®¤ä¸­ç­‰å›°æƒ‘åº¦
            
        except Exception as e:
            print(f"è®¡ç®—å›°æƒ‘åº¦æ—¶å‡ºé”™: {e}")
            return 50.0
    
    def analyze_document(self, latex_content: str, window_size: int = 50) -> List[Tuple[str, float]]:
        """
        åˆ†ææ•´ä¸ªæ–‡æ¡£ï¼Œè®¡ç®—æ¯ä¸ªè¯å…ƒçš„å›°æƒ‘åº¦
        
        Args:
            latex_content: LaTeX æºä»£ç 
            window_size: ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆè¯æ•°ï¼‰
            
        Returns:
            List of (token, perplexity) tuples
        """
        tokens = self.extract_text_from_latex(latex_content)
        results = []
        
        print(f"å¼€å§‹åˆ†æï¼Œå…± {len(tokens)} ä¸ªè¯å…ƒ...")
        
        for i, token in enumerate(tokens):
            # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆå‰ window_size ä¸ªè¯ï¼‰
            start_idx = max(0, i - window_size)
            context = ' '.join(tokens[start_idx:i])
            
            if len(context.strip()) == 0:
                # ç¬¬ä¸€ä¸ªè¯ï¼Œå›°æƒ‘åº¦è®¾ä¸ºä¸­ç­‰
                perplexity = 50.0
            else:
                perplexity = self.calculate_token_perplexity(context, token)
            
            results.append((token, perplexity))
            
            if (i + 1) % 10 == 0:
                print(f"å·²å¤„ç† {i + 1}/{len(tokens)} ä¸ªè¯å…ƒ...")
        
        return results
    
    def perplexity_to_color(self, perplexity: float) -> str:
        """
        å°†å›°æƒ‘åº¦æ˜ å°„åˆ°é¢œè‰²
        å›°æƒ‘åº¦è¶Šä½ï¼ŒAI ç”Ÿæˆæ¦‚ç‡è¶Šé«˜ï¼Œé¢œè‰²è¶Šçº¢
        
        Args:
            perplexity: å›°æƒ‘åº¦å€¼
            
        Returns:
            LaTeX é¢œè‰²ä»£ç 
        """
        # å½’ä¸€åŒ–å›°æƒ‘åº¦åˆ° [0, 1] åŒºé—´
        # å‡è®¾å›°æƒ‘åº¦èŒƒå›´æ˜¯ [1, 100]
        if perplexity <= 1:
            normalized = 0.0
        elif perplexity >= 100:
            normalized = 1.0
        else:
            # ä½¿ç”¨å¯¹æ•°å°ºåº¦
            normalized = np.log(perplexity) / np.log(100)
        
        # åè½¬ï¼šå›°æƒ‘åº¦ä½ -> normalized ä½ -> AI æ¦‚ç‡é«˜
        ai_prob = 1.0 - normalized
        
        if ai_prob >= 0.75:
            return "red"  # é«˜ AI æ¦‚ç‡
        elif ai_prob >= 0.5:
            return "orange"  # ä¸­é«˜ AI æ¦‚ç‡
        elif ai_prob >= 0.25:
            return "yellow"  # ä¸­ç­‰ AI æ¦‚ç‡
        elif ai_prob >= 0.1:
            return "green"  # ä½ AI æ¦‚ç‡
        else:
            return "blue"  # å¾ˆä½ AI æ¦‚ç‡
    
    def generate_highlighted_latex(self, latex_content: str, analysis_results: List[Tuple[str, float]]) -> str:
        """
        ç”Ÿæˆå¸¦é¢œè‰²é«˜äº®çš„ LaTeX æ–‡æ¡£
        
        Args:
            latex_content: åŸå§‹ LaTeX å†…å®¹
            analysis_results: åˆ†æç»“æœ
            
        Returns:
            å¸¦é«˜äº®çš„ LaTeX å†…å®¹
        """
        # åœ¨æ–‡æ¡£å¼€å¤´æ·»åŠ å¿…è¦çš„åŒ…
        preamble = r"""\usepackage{xcolor}
\usepackage{soul}

% å®šä¹‰é«˜äº®å‘½ä»¤
\newcommand{\hlblue}[1]{\sethlcolor{blue!20}\hl{#1}}
\newcommand{\hlgreen}[1]{\sethlcolor{green!20}\hl{#1}}
\newcommand{\hlyellow}[1]{\sethlcolor{yellow!40}\hl{#1}}
\newcommand{\hlorange}[1]{\sethlcolor{orange!40}\hl{#1}}
\newcommand{\hlred}[1]{\sethlcolor{red!40}\hl{#1}}

"""
        
        # æŸ¥æ‰¾ \begin{document} å¹¶åœ¨å…¶å‰æ’å…¥ preamble
        if r'\begin{document}' in latex_content:
            parts = latex_content.split(r'\begin{document}', 1)
            highlighted_content = parts[0] + preamble + r'\begin{document}' + parts[1]
        else:
            highlighted_content = preamble + latex_content
        
        # ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„è¯å…ƒæ·»åŠ é«˜äº®
        # è¿™éƒ¨åˆ†éœ€è¦æ›´å¤æ‚çš„å®ç°æ¥å‡†ç¡®å®šä½åŸæ–‡ä¸­çš„è¯å…ƒ
        # ç®€åŒ–ç‰ˆæœ¬ï¼šæ·»åŠ å›¾ä¾‹
        legend = r"""
\vspace{1cm}
\noindent\textbf{AI æ£€æµ‹å›¾ä¾‹ï¼š}\\
\hlblue{è“è‰²} - æä½ AI æ¦‚ç‡ \quad
\hlgreen{ç»¿è‰²} - ä½ AI æ¦‚ç‡ \quad
\hlyellow{é»„è‰²} - ä¸­ç­‰ AI æ¦‚ç‡ \quad
\hlorange{æ©™è‰²} - é«˜ AI æ¦‚ç‡ \quad
\hlred{çº¢è‰²} - æé«˜ AI æ¦‚ç‡

\vspace{0.5cm}
"""
        
        # åœ¨ \begin{document} åæ·»åŠ å›¾ä¾‹
        highlighted_content = highlighted_content.replace(
            r'\begin{document}',
            r'\begin{document}' + '\n' + legend
        )
        
        return highlighted_content
    
    def process_file(self, input_file: str, output_file: str, window_size: int = 50):
        """
        å¤„ç† LaTeX æ–‡ä»¶
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            window_size: ä¸Šä¸‹æ–‡çª—å£å¤§å°
        """
        print(f"è¯»å–æ–‡ä»¶: {input_file}")
        
        with open(input_file, 'r', encoding='utf-8') as f:
            latex_content = f.read()
        
        print("å¼€å§‹åˆ†ææ–‡æ¡£...")
        analysis_results = self.analyze_document(latex_content, window_size)
        
        print("ç”Ÿæˆå¸¦é«˜äº®çš„ LaTeX æ–‡æ¡£...")
        highlighted_latex = self.generate_highlighted_latex(latex_content, analysis_results)
        
        print(f"ä¿å­˜ç»“æœåˆ°: {output_file}")
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(highlighted_latex)
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        perplexities = [p for _, p in analysis_results]
        print("\n=== ç»Ÿè®¡æŠ¥å‘Š ===")
        print(f"æ€»è¯å…ƒæ•°: {len(analysis_results)}")
        print(f"å¹³å‡å›°æƒ‘åº¦: {np.mean(perplexities):.2f}")
        print(f"å›°æƒ‘åº¦ä¸­ä½æ•°: {np.median(perplexities):.2f}")
        print(f"æœ€ä½å›°æƒ‘åº¦: {np.min(perplexities):.2f}")
        print(f"æœ€é«˜å›°æƒ‘åº¦: {np.max(perplexities):.2f}")
        
        # ç»Ÿè®¡å„é¢œè‰²åŒºé—´çš„è¯å…ƒæ•°
        colors = [self.perplexity_to_color(p) for _, p in analysis_results]
        color_counts = {
            'blue': colors.count('blue'),
            'green': colors.count('green'),
            'yellow': colors.count('yellow'),
            'orange': colors.count('orange'),
            'red': colors.count('red')
        }
        
        print("\né¢œè‰²åˆ†å¸ƒ:")
        for color, count in color_counts.items():
            percentage = (count / len(colors)) * 100 if colors else 0
            print(f"{color}: {count} ({percentage:.1f}%)")
        
        print("\nå®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(
        description='LaTeX AI å†…å®¹æ£€æµ‹å™¨ - é€šè¿‡å›°æƒ‘åº¦åˆ†æè¯†åˆ«å¯èƒ½ç”± AI ç”Ÿæˆçš„å†…å®¹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨ OpenAI API
  python aidetector.py paper.tex -k YOUR_API_KEY
  
  # ä½¿ç”¨ LMStudio æœ¬åœ°æ¨¡å‹ï¼ˆæ¨èï¼Œå…è´¹ï¼‰
  python aidetector.py paper.tex --lmstudio
  
  # æŒ‡å®š LMStudio æœåŠ¡å™¨åœ°å€
  python aidetector.py paper.tex --lmstudio --lmstudio-url http://localhost:1234/v1
        """
    )
    parser.add_argument('input', help='è¾“å…¥çš„ LaTeX æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºçš„ LaTeX æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä¸º input_highlighted.texï¼‰')
    parser.add_argument('-k', '--api-key', help='OpenAI API å¯†é’¥ï¼ˆä¹Ÿå¯é€šè¿‡ OPENAI_API_KEY ç¯å¢ƒå˜é‡è®¾ç½®ï¼‰')
    parser.add_argument('-m', '--model', default='gpt-3.5-turbo', help='ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤: gpt-3.5-turboï¼‰')
    parser.add_argument('-w', '--window', type=int, default=50, help='ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆé»˜è®¤: 50ï¼‰')
    parser.add_argument('--lmstudio', action='store_true', help='ä½¿ç”¨ LMStudio æœ¬åœ°æ¨¡å‹ï¼ˆå…è´¹ï¼Œéœ€å…ˆå¯åŠ¨ LMStudio æœåŠ¡ï¼‰')
    parser.add_argument('--lmstudio-url', default='http://localhost:1234/v1', help='LMStudio æœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤: http://localhost:1234/v1ï¼‰')
    
    args = parser.parse_args()
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
    if args.output:
        output_file = args.output
    else:
        base_name = os.path.splitext(args.input)[0]
        output_file = f"{base_name}_highlighted.tex"
    
    # åˆ›å»ºæ£€æµ‹å™¨å¹¶å¤„ç†æ–‡ä»¶
    try:
        detector = LaTeXAIDetector(
            api_key=args.api_key, 
            model=args.model,
            use_lmstudio=args.lmstudio,
            lmstudio_url=args.lmstudio_url
        )
        detector.process_file(args.input, output_file, args.window)
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        return 1
    
    return 0


if __name__ == '__main__':
    exit(main())
